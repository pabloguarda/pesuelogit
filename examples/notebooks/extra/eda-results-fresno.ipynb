{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import isuelogit as isl\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main dir: /Users/pablo/github/pesuelogit\n"
     ]
    }
   ],
   "source": [
    "# Path management\n",
    "main_dir = str(Path(os.path.abspath('')).parents[1])\n",
    "os.chdir(main_dir)\n",
    "print('main dir:', main_dir)\n",
    "\n",
    "sys.path.append(os.path.join(main_dir, 'src'))\n",
    "\n",
    "isl.config.dirs['read_network_data'] = \"input/network-data/fresno/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Internal modules\n",
    "from pesuelogit.visualizations import plot_predictive_performance, plot_convergence_estimates\n",
    "from pesuelogit.etl import data_curation, add_period_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read spatiotemporal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folderpath = isl.config.dirs['read_network_data'] + 'links/spatiotemporal-data/'\n",
    "df = pd.concat([pd.read_csv(file) for file in glob.glob(folderpath + \"*link-data*\")], axis=0)\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')\n",
    "df = df[df['date'].dt.dayofweek.between(0, 3)]\n",
    "# df = df[df['date'].dt.year == 2019]\n",
    "\n",
    "df['period'] = df['date'].astype(str) + '-' + df['hour'].astype(str)\n",
    "df['period'] = df.period.map(hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add period id for time-varying estimation\n",
    "\n",
    "period_feature = 'hour'\n",
    "\n",
    "df['period'] = df['date'].astype(str) + '-' + df[period_feature].astype(str)\n",
    "# df['period'] = df.period.map(hash)\n",
    "\n",
    "df = add_period_id(df, period_feature='hour')\n",
    "\n",
    "period_keys = df[[period_feature,'period_id']].drop_duplicates().reset_index().drop('index',axis =1).sort_values('hour')\n",
    "print(period_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tt_ff'] = np.where(df['link_type'] != 'LWRLK', 0,df['length']/df['speed_ref_avg'])\n",
    "df.loc[(df.link_type == \"LWRLK\") & (df.speed_ref_avg == 0),'tt_ff'] = float('nan')\n",
    "\n",
    "df['tt_avg'] = np.where(df['link_type'] != 'LWRLK', 0,df['length']/df['speed_hist_avg'])\n",
    "df.loc[(df.link_type == \"LWRLK\") & (df.speed_hist_avg == 0),'tt_avg'] = float('nan')\n",
    "\n",
    "df = data_curation(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['speed_ref_avg','speed_hist_avg', 'tt_ff', 'tt_avg']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')\n",
    "df['year'] = df.date.dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query('year == 2019')[['counts', 'tt_ff', 'tt_avg', 'tf_inrix', 'speed_avg']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query('year == 2020')[['counts', 'tt_ff', 'tt_avg', 'tf_inrix', 'speed_avg']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1-1793.271103/1865.514775, 1-17.469430/18.895705)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single period models\n",
    "df.loc[(df.hour == 16) & (df.year == 2019),['counts', 'tt_avg']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TVODLULPE\n",
    "df.loc[(df.hour.isin([6,7,8,15,16,17])) & (df.year == 2019),['counts', 'tt_avg']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_Z = ['tt_sd', 'median_inc', 'incidents', 'bus_stops', 'intersections']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_plot = ['speed_sd', 'median_inc', 'incidents', 'bus_stops', 'intersections']\n",
    "\n",
    "sns.lineplot(x= 'date', y = 'value', hue = 'variable', data =pd.melt(df.groupby('date')[features_plot].mean().reset_index(),id_vars= ['date']))\n",
    "plt.tight_layout()\n",
    "plt.legend(title=\"\")\n",
    "plt.ylabel(\"value of exogenous feature\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze single file of inrix data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inrix_df = pd.read_csv(f\"{os.getcwd()}/input/private/inrix/2020-10-01.csv\")\n",
    "inrix_df['ts'] = pd.to_datetime(inrix_df['UTC Date Time'])\n",
    "inrix_df['hour'] = inrix_df.ts.dt.hour\n",
    "inrix_df['speed'] = inrix_df['Speed(km/hour)']*0.62137119223"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select data from same time range\n",
    "inrix_df = inrix_df[inrix_df.hour.isin(range(4,23))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inrix_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inrix_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2, figsize = (10,4))\n",
    "\n",
    "sns.lineplot(data = inrix_df.groupby('hour')[['Speed(km/hour)']].mean(),\n",
    "             ax = axs[0])\n",
    "axs[0].set_title('avg of speed')\n",
    "\n",
    "sns.lineplot(data = inrix_df.groupby('hour')[['speed']].std(),\n",
    "             ax = axs[1])\n",
    "axs[1].set_title('std of speed');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter only data from inrix segments that were matched with the network links\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize = (10,4))\n",
    "\n",
    "sns.lineplot(data = inrix_df[inrix_df['Segment ID'].isin(list(df.inrix_id.dropna().unique()))].groupby('hour')[['speed']].mean(),\n",
    "             ax = axs[0])\n",
    "axs[0].set_title('avg of speed')\n",
    "\n",
    "sns.lineplot(data = inrix_df[inrix_df['Segment ID'].isin(list(df.inrix_id.dropna().unique()))].groupby('hour')[['speed']].std(),\n",
    "             ax = axs[1])\n",
    "axs[1].set_title('std of speed');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check that there is a balanced amount of observations per date\n",
    "obs_date = df.groupby('date')['hour'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats by date\n",
    "df.groupby('date')[['speed_sd','speed_avg', 'counts']].mean().assign(total_obs = obs_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross sectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_df = df[df.year == 2019].copy()\n",
    "#eda_df = df.copy()\n",
    "\n",
    "eda_df['day'] = eda_df.date.dt.day#.astype(str)\n",
    "eda_df['hour_id'] = eda_df['hour'].astype(str).apply(lambda x: time.strftime(\"%l%p\", time.strptime(x,\"%H\")))\n",
    "eda_df['date'] = eda_df['date'].astype(str)\n",
    "\n",
    "# Transform to monthly income\n",
    "eda_df['median_inc'] = eda_df['median_inc']/12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "eda_df['hour_id'] = pd.Categorical(eda_df['hour_id'], categories = eda_df[['hour_id', 'hour']].drop_duplicates().sort_values('hour')['hour_id'], ordered = True)\n",
    "\n",
    "eda_df['day'] = pd.Categorical(eda_df['day'], categories = eda_df[['day']].drop_duplicates().sort_values('day')['day'], ordered = True)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "eda_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "eda_df.groupby('day')[['counts']].mean().reset_index()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "daily_counts = eda_df.groupby('day')[['counts']].mean().reset_index()\n",
    "daily_counts['day'] = daily_counts['day'].astype(str)\n",
    "\n",
    "daily_speed = eda_df.groupby('day')[['speed_avg']].mean().reset_index()\n",
    "daily_speed['day']  = daily_speed['day'].astype(str)\n",
    "\n",
    "daily_attributes = pd.melt(eda_df.groupby('day')[features_plot].mean().reset_index(),id_vars= ['day'])\n",
    "daily_attributes['day']  = daily_attributes['day'].astype(str)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "daily_attributes['day']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#formatter = matplotlib.days.dayFormatter('%d')\n",
    "#locator = matplotlib.dates.DayLocator(interval = 2)\n",
    "\n",
    "fig, axs = plt.subplots(1,3, figsize = (12,4))\n",
    "\n",
    "sns.lineplot(x= 'day', y = 'counts', data = daily_counts,\n",
    "             ax = axs[0])\n",
    "\n",
    "sns.lineplot(x= 'day', y = 'speed_avg', data =daily_speed,\n",
    "             ax = axs[1])\n",
    "\n",
    "sns.lineplot(x= 'day', y = 'value', hue = 'variable', data =daily_attributes,\n",
    "             ax = axs[2])\n",
    "\n",
    "for ax in axs:\n",
    "#     ax.set_xticks(np.arange(1,31,2))\n",
    "#     ax.set_xticklabels(np.arange(1,31,2))\n",
    "    #ax.xaxis.set_major_formatter(formatter)\n",
    "    #ax.xaxis.set_major_locator(locator)\n",
    "    ax.set_xlabel('day of the month')\n",
    "\n",
    "axs[0].set_ylabel('link flow [vehicles per hour]')\n",
    "axs[1].set_ylabel('speed [miles per hour]')\n",
    "\n",
    "axs[2].legend(title=\"\", loc = 'upper right')\n",
    "axs[2].set_ylabel(\"value of exogenous attribute\")\n",
    "\n",
    "#list(map(lambda x: x.set_xticklabels(x.get_xticks(), rotation=0), axs));\n",
    "#fig.autofmt_xdate(rotation = 90, ha = 'center')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By hour of day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_keys = eda_df[(eda_df.counts>0) & (eda_df.speed_avg>0)].link_key.unique()\n",
    "link_keys = link_keys[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2, figsize = (10,4))\n",
    "\n",
    "sns.lineplot(x= 'hour', y = 'counts', hue = 'link_key',\n",
    "             data =eda_df[eda_df.link_key.isin(link_keys)].groupby(['hour','link_key'])[['counts']].mean().reset_index(),\n",
    "             ax = axs[0])\n",
    "\n",
    "sns.lineplot(x= 'hour', y = 'speed_avg', hue = 'link_key',\n",
    "             data =eda_df[eda_df.link_key.isin(link_keys)].groupby(['hour','link_key'])[['speed_avg']].mean().reset_index(),\n",
    "             ax = axs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis for links where link counts are reported\n",
    "\n",
    "fig, axs = plt.subplots(1,4, figsize = (20,4))\n",
    "\n",
    "sns.lineplot(x= 'hour', y = 'counts', data =eda_df.groupby(['hour'])[['counts']].mean().reset_index(),\n",
    "             ax = axs[0])\n",
    "\n",
    "sns.lineplot(x= 'hour', y = 'speed_avg',\n",
    "             data =eda_df[~eda_df.counts.isna()].groupby(['hour'])[['speed_avg']].mean().reset_index(),\n",
    "             ax = axs[1])\n",
    "\n",
    "sns.lineplot(x= 'hour', y = 'speed_max',\n",
    "             data =eda_df[~eda_df.counts.isna()].groupby(['hour'])[['speed_max']].max().reset_index(),\n",
    "             ax = axs[2])\n",
    "\n",
    "sns.lineplot(x= 'hour', y = 'speed_sd',\n",
    "             data =eda_df[~eda_df.counts.isna()].groupby(['hour'])[['speed_sd']].mean().reset_index(),\n",
    "             ax = axs[3])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Select data from 2019 only\n",
    "eda_df = eda_df[eda_df.year == 2019]\n",
    "eda_df['hour_id'] = pd.Categorical(eda_df['hour_id'], categories = eda_df[['hour_id', 'hour']].drop_duplicates().sort_values('hour')['hour_id'], ordered = True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis for links where link counts are reported\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize = (9,4))\n",
    "\n",
    "sns.lineplot(x= 'hour_id', y = 'counts', data =eda_df.groupby(['hour_id'])[['counts']].mean().reset_index(),\n",
    "             ax = axs[0])\n",
    "\n",
    "sns.lineplot(x= 'hour_id', y = 'speed_avg',\n",
    "             data =eda_df[~eda_df.counts.isna()].groupby(['hour_id'])[['speed_avg']].mean().reset_index(),\n",
    "             ax = axs[1])\n",
    "fig.autofmt_xdate(rotation = 90, ha = 'center')\n",
    "\n",
    "axs[0].set_ylabel('average link flow [vehicles per hour]')\n",
    "axs[1].set_ylabel('average speed [miles per hour]')\n",
    "list(map(lambda x: x.set_xlabel('hour'),axs))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read models results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = 230211161720\n",
    "train_results_dfs = pd.read_csv(f'output/tables/{ts}_train_results_Fresno.csv', index_col = [0])\n",
    "test_results_dfs = pd.read_csv(f'output/tables/{ts}_train_results_Fresno.csv', index_col = [0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_XTICKS_SPACING = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Estimation of utility function (LUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nLUE: Estimation of utility function')\n",
    "\n",
    "# Average reliability ratio over epochs\n",
    "plot_convergence_estimates(estimates=train_results_dfs[train_results_dfs.model == 'lue'].\\\n",
    "                   assign(rr = train_results_dfs[train_results_dfs.model == 'lue']['tt_sd']/train_results_dfs[train_results_dfs.model == 'lue']['tt'])[['epoch','rr']],\n",
    "                       xticks_spacing = _XTICKS_SPACING)\n",
    "\n",
    "plot_predictive_performance(train_losses=train_results_dfs[train_results_dfs.model == 'lue'],\n",
    "                            val_losses=test_results_dfs[test_results_dfs.model == 'lue'],\n",
    "                            xticks_spacing = _XTICKS_SPACING)\n",
    "\n",
    "# print(f\"theta = {dict(zip(utility_parameters.true_values.keys(), list(lue.theta.numpy())))}\")\n",
    "print(f\"alpha = {list(train_results_dfs[train_results_dfs.model == 'lue']['alpha'])[-1]: 0.2f}, \"\n",
    "      f\"beta  = {list(train_results_dfs[train_results_dfs.model == 'lue']['beta'])[-1]: 0.2f}\")\n",
    "# print(f\"Avg abs diff of observed and estimated OD: {np.mean(np.abs(lue.q - fresno_network.q.flatten())): 0.2f}\")\n",
    "# print(f\"Avg observed OD: {np.mean(np.abs(fresno_network.q.flatten())): 0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: OD + utility estimation with historic OD (ODLUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nODLUE: OD + utility estimation with historic OD')\n",
    "\n",
    "train_results_dfs.loc[train_results_dfs.model == 'odlue','model'] = 'odlue'\n",
    "test_results_dfs.loc[test_results_dfs.model == 'odlue','model'] = 'odlue'\n",
    "\n",
    "# Average reliability ratio over epochs\n",
    "plot_convergence_estimates(estimates=train_results_dfs[train_results_dfs.model == 'odlue'].\\\n",
    "                   assign(rr = train_results_dfs[train_results_dfs.model == 'odlue']['tt_sd']/train_results_dfs[train_results_dfs.model == 'odlue']['tt'])[['epoch','rr']],\n",
    "                       xticks_spacing = _XTICKS_SPACING)\n",
    "\n",
    "plot_predictive_performance(train_losses=train_results_dfs[train_results_dfs.model == 'odlue'],\n",
    "                            val_losses=test_results_dfs[test_results_dfs.model == 'odlue'],\n",
    "                            show_validation= False,\n",
    "                            xticks_spacing = _XTICKS_SPACING)\n",
    "\n",
    "\n",
    "# print(f\"theta = {dict(zip(utility_parameters.true_values.keys(), list(odlue_1.theta.numpy())))}\")\n",
    "print(f\"alpha = {list(train_results_dfs[train_results_dfs.model == 'odlue']['alpha'])[-1]: 0.2f}, \"\n",
    "      f\"beta  = {list(train_results_dfs[train_results_dfs.model == 'odlue']['beta'])[-1]: 0.2f}\")\n",
    "# print(f\"Avg abs diff of observed and estimated OD: {np.mean(np.abs(odlue_1.q - fresno_network.q.flatten())): 0.2f}\")\n",
    "# print(f\"Avg observed OD: {np.mean(np.abs(fresno_network.q.flatten())): 0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: ODLUE + link specific performance parameters (ODLULPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nODLULPE: ODLUE + link performance parameters with historic OD matrix (link specifics alphas and betas)')\n",
    "\n",
    "train_results_dfs.loc[train_results_dfs.model == 'odlulpe','model'] = 'odlulpe'\n",
    "test_results_dfs.loc[test_results_dfs.model == 'odlulpe','model'] = 'odlulpe'\n",
    "\n",
    "# Average reliability ratio over epochs\n",
    "plot_convergence_estimates(estimates=train_results_dfs[train_results_dfs.model == 'odlulpe'].\\\n",
    "                   assign(rr = train_results_dfs[train_results_dfs.model == 'odlulpe']['tt_sd']/train_results_dfs[train_results_dfs.model == 'odlulpe']['tt'])[['epoch','rr']],\n",
    "                       xticks_spacing = _XTICKS_SPACING)\n",
    "\n",
    "plot_predictive_performance(train_losses=train_results_dfs[train_results_dfs.model == 'odlulpe'],\n",
    "                            val_losses=test_results_dfs[test_results_dfs.model == 'odlulpe'],\n",
    "                            show_validation= False,\n",
    "                            xticks_spacing = _XTICKS_SPACING)\n",
    "\n",
    "plot_convergence_estimates(\n",
    "    estimates=train_results_dfs[train_results_dfs.model == 'odlulpe'][['epoch','alpha','beta']],\n",
    "    xticks_spacing = _XTICKS_SPACING)\n",
    "\n",
    "# print(f\"theta = {dict(zip(utility_parameters.true_values.keys(), list(odlulpe_1.theta.numpy())))}\")\n",
    "print(f\"alpha = {list(train_results_dfs[train_results_dfs.model == 'odlulpe']['alpha'])[-1]: 0.2f}, \"\n",
    "      f\"beta  = {list(train_results_dfs[train_results_dfs.model == 'odlulpe']['beta'])[-1]: 0.2f}\")\n",
    "# print(f\"Avg abs diff of observed and estimated OD: {np.mean(np.abs(odlulpe_1.q - fresno_network.q.flatten())): 0.2f}\")\n",
    "# print(f\"Avg observed OD: {np.mean(np.abs(fresno_network.q.flatten())): 0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: ODLULPE with Time Varying OD and Utility Function (TVODLULPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\ntvodlulpe: Time specific utility and OD, link performance parameters')\n",
    "\n",
    "train_results_dfs.loc[train_results_dfs.model == 'tvodlulpe','model'] = 'tvodlulpe'\n",
    "test_results_dfs.loc[test_results_dfs.model == 'tvodlulpe','model'] = 'tvodlulpe'\n",
    "\n",
    "# Average reliability ratio over epochs\n",
    "plot_convergence_estimates(estimates=train_results_dfs[train_results_dfs.model == 'tvodlulpe'].\\\n",
    "                   assign(rr = train_results_dfs[train_results_dfs.model == 'tvodlulpe']['tt_sd']/train_results_dfs[train_results_dfs.model == 'tvodlulpe']['tt'])[['epoch','rr']],\n",
    "                       xticks_spacing = _XTICKS_SPACING)\n",
    "\n",
    "plot_predictive_performance(train_losses=train_results_dfs[train_results_dfs.model == 'tvodlulpe'],\n",
    "                            val_losses=test_results_dfs[test_results_dfs.model == 'tvodlulpe'],\n",
    "                            show_validation= False,\n",
    "                            xticks_spacing = _XTICKS_SPACING)\n",
    "\n",
    "plot_convergence_estimates(\n",
    "    estimates=train_results_dfs[train_results_dfs.model == 'tvodlulpe'][['epoch','alpha','beta']],\n",
    "    xticks_spacing = _XTICKS_SPACING)\n",
    "\n",
    "# print(f\"theta = {dict(zip(utility_parameters.true_values.keys(), list(tvodlulpe_1.theta.numpy())))}\")\n",
    "print(f\"alpha = {list(train_results_dfs[train_results_dfs.model == 'tvodlulpe']['alpha'])[-1]: 0.2f}, \"\n",
    "      f\"beta  = {list(train_results_dfs[train_results_dfs.model == 'tvodlulpe']['beta'])[-1]: 0.2f}\")\n",
    "# print(f\"Avg abs diff of observed and estimated OD: {np.mean(np.abs(tvodlulpe_1.q - fresno_network.q.flatten())): 0.2f}\")\n",
    "# print(f\"Avg observed OD: {np.mean(np.abs(fresno_network.q.flatten())): 0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of parameters estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_EPOCHS = {'learning': 150, 'equilibrium': 50}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_results_dfs = train_results_dfs[train_results_dfs['epoch'] <= _EPOCHS['learning']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = train_results_dfs.model.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({'parameter': [], 'model': []})\n",
    "\n",
    "for model in models:\n",
    "    results_model = train_results_dfs.loc[train_results_dfs.model == model].iloc[-1]\n",
    "    results = results.append(pd.DataFrame(\n",
    "        {'parameter': ['tt'] + features_Z +\n",
    "                      ['fixed_effect_mean','fixed_effect_std',\n",
    "                       'alpha_mean', 'alpha_std',\n",
    "                       'beta_mean', 'beta_std',\n",
    "                       # 'od_mean', 'od_std'\n",
    "                       ],\n",
    "         'values': list(results_model[['tt'] + features_Z]) +\n",
    "                   [np.mean(results_model['fixed_effect']),np.std(results_model['fixed_effect']),\n",
    "                    np.mean(results_model['alpha']),np.std(results_model['alpha']),\n",
    "                    np.mean(results_model['beta']),np.std(results_model['beta']),\n",
    "                    # np.mean(model.q),np.std(model.q)\n",
    "                    ]\n",
    "         }\n",
    "    ).assign(model = model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.pivot_table(index = ['parameter'], columns = 'model', values = 'values', sort=False).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_estimates = {}\n",
    "train_losses = {}\n",
    "\n",
    "for model in models:\n",
    "    train_estimates[model] = train_results_dfs[train_results_dfs.model == model]\n",
    "\n",
    "    train_estimates[model]['model'] = model\n",
    "\n",
    "train_estimates_df = pd.concat(train_estimates.values())\n",
    "\n",
    "train_estimates_df['rr'] = train_estimates_df['tt_sd']/train_estimates_df['tt']\n",
    "\n",
    "estimates = train_estimates_df[['epoch','model','rr']].reset_index().drop('index',axis = 1)\n",
    "#estimates = estimates[estimates.epoch != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1)\n",
    "\n",
    "g = sns.lineplot(data=estimates, x='epoch', hue='model', y='rr')\n",
    "\n",
    "ax.set_ylabel('average reliability ratio')\n",
    "\n",
    "plt.ylim(ymin=0)\n",
    "\n",
    "ax.set_xticks(np.arange(estimates['epoch'].min(), estimates['epoch'].max() + 1, _XTICKS_SPACING))\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pesuelogit",
   "language": "python",
   "name": "pesuelogit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
