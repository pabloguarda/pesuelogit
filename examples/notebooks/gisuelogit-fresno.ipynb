{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import isuelogit as isl\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main dir: /Users/pablo/github/gisuelogit\n"
     ]
    }
   ],
   "source": [
    "# Path management\n",
    "main_dir = str(Path(os.path.abspath('')).parents[1])\n",
    "os.chdir(main_dir)\n",
    "print('main dir:', main_dir)\n",
    "\n",
    "isl.config.dirs['read_network_data'] = \"input/network-data/fresno/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Internal modules\n",
    "from src.gisuelogit.models import UtilityParameters, BPRParameters, ODParameters, GISUELOGIT, compute_rr\n",
    "from src.gisuelogit.visualizations import plot_predictive_performance, plot_convergence_estimates, plot_top_od_flows_periods, plot_utility_parameters_periods, plot_rr_by_period, plot_rr_by_period_models, plot_total_trips_models\n",
    "from src.gisuelogit.networks import load_k_shortest_paths, read_paths, build_fresno_network, \\\n",
    "    Equilibrator, sparsify_OD, ColumnGenerator, read_OD\n",
    "from src.gisuelogit.etl import get_design_tensor, get_y_tensor, data_curation, temporal_split, add_period_id, get_tensors_by_year\n",
    "from src.gisuelogit.descriptive_statistics import mse, btcg_mse, mnrmse, nrmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed for reproducibility\n",
    "_SEED = 2023\n",
    "np.random.seed(_SEED)\n",
    "random.seed(_SEED)\n",
    "tf.random.set_seed(_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Fresno network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fresno_network = build_fresno_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read OD matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Q (1789, 1789) read in 0.0[s] with sparse format\n",
      "66266.3 trips were loaded among 6970 o-d pairs\n"
     ]
    }
   ],
   "source": [
    "read_OD(network=fresno_network, sparse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Read paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading paths: |################----| 81.5% "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [7], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m#read_paths(network=fresno_network, update_incidence_matrices=True, filename='paths-fresno.csv')\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mread_paths\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnetwork\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfresno_network\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mupdate_incidence_matrices\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mpaths-full-model-fresno.csv\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# For quick testing\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# Q = fresno_network.load_OD(sparsify_OD(fresno_network.Q, prop_od_pairs=0.99))\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# load_k_shortest_paths(network=fresno_network, k=2, update_incidence_matrices=True)\u001B[39;00m\n",
      "File \u001B[0;32m~/github/gisuelogit/src/gisuelogit/networks.py:357\u001B[0m, in \u001B[0;36mread_paths\u001B[0;34m(network, **kwargs)\u001B[0m\n\u001B[1;32m    356\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mread_paths\u001B[39m(network, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 357\u001B[0m     \u001B[43misl\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfactory\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mPathsGenerator\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_paths\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnetwork\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnetwork\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/gisuelogit-qeH6w6zX-py3.9/lib/python3.9/site-packages/isuelogit/factory.py:976\u001B[0m, in \u001B[0;36mPathsGenerator.read_paths\u001B[0;34m(self, network, **kwargs)\u001B[0m\n\u001B[1;32m    969\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mread_paths\u001B[39m(\u001B[38;5;28mself\u001B[39m, network, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    970\u001B[0m \n\u001B[1;32m    971\u001B[0m     \u001B[38;5;66;03m# options = self.get_updated_options(**{'reading':{'paths':True}})\u001B[39;00m\n\u001B[1;32m    972\u001B[0m     \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m    973\u001B[0m     \u001B[38;5;66;03m# if options['reading']['paths']:\u001B[39;00m\n\u001B[1;32m    974\u001B[0m     \u001B[38;5;66;03m#     # print('reading paths')\u001B[39;00m\n\u001B[0;32m--> 976\u001B[0m     paths \u001B[38;5;241m=\u001B[39m \u001B[43mread_internal_paths\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnetwork\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnetwork\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    977\u001B[0m \u001B[43m                                \u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mfilename\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    978\u001B[0m \u001B[43m                                \u001B[49m\u001B[43mfolderpath\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mfolderpath\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    979\u001B[0m     network\u001B[38;5;241m.\u001B[39mload_paths(paths\u001B[38;5;241m=\u001B[39mpaths, update_incidence_matrices\u001B[38;5;241m=\u001B[39mkwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mupdate_incidence_matrices\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m))\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/gisuelogit-qeH6w6zX-py3.9/lib/python3.9/site-packages/isuelogit/reader.py:154\u001B[0m, in \u001B[0;36mread_internal_paths\u001B[0;34m(network, folderpath, filename)\u001B[0m\n\u001B[1;32m    150\u001B[0m \u001B[38;5;66;03m# Generate a list of links from each line depicting a certain path\u001B[39;00m\n\u001B[1;32m    152\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m path_line, counter \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(path_reader, \u001B[38;5;28mrange\u001B[39m(total_paths)):\n\u001B[0;32m--> 154\u001B[0m     \u001B[43mprinter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprintProgressBar\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcounter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtotal_paths\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprefix\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mReading paths:\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuffix\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mlength\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    156\u001B[0m     links \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m    158\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(path_line) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m    159\u001B[0m         \u001B[38;5;66;03m# links = links.append(Link(label=(path_line[i], path_line[i + 1])))\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/gisuelogit-qeH6w6zX-py3.9/lib/python3.9/site-packages/isuelogit/printer.py:52\u001B[0m, in \u001B[0;36mprintProgressBar\u001B[0;34m(iteration, total, prefix, suffix, decimals, length, eraseBar)\u001B[0m\n\u001B[1;32m     49\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     50\u001B[0m         sys\u001B[38;5;241m.\u001B[39mstdout\u001B[38;5;241m.\u001B[39mwrite(\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 52\u001B[0m \u001B[43msys\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstdout\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflush\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/gisuelogit-qeH6w6zX-py3.9/lib/python3.9/site-packages/ipykernel/iostream.py:488\u001B[0m, in \u001B[0;36mOutStream.flush\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    486\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpub_thread\u001B[38;5;241m.\u001B[39mschedule(evt\u001B[38;5;241m.\u001B[39mset)\n\u001B[1;32m    487\u001B[0m     \u001B[38;5;66;03m# and give a timeout to avoid\u001B[39;00m\n\u001B[0;32m--> 488\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[43mevt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflush_timeout\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m    489\u001B[0m         \u001B[38;5;66;03m# write directly to __stderr__ instead of warning because\u001B[39;00m\n\u001B[1;32m    490\u001B[0m         \u001B[38;5;66;03m# if this is happening sys.stderr may be the problem.\u001B[39;00m\n\u001B[1;32m    491\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIOStream.flush timed out\u001B[39m\u001B[38;5;124m\"\u001B[39m, file\u001B[38;5;241m=\u001B[39msys\u001B[38;5;241m.\u001B[39m__stderr__)\n\u001B[1;32m    492\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/threading.py:574\u001B[0m, in \u001B[0;36mEvent.wait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    572\u001B[0m signaled \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_flag\n\u001B[1;32m    573\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m signaled:\n\u001B[0;32m--> 574\u001B[0m     signaled \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_cond\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    575\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m signaled\n",
      "File \u001B[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/threading.py:316\u001B[0m, in \u001B[0;36mCondition.wait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    314\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    315\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m--> 316\u001B[0m         gotit \u001B[38;5;241m=\u001B[39m \u001B[43mwaiter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43macquire\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    317\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    318\u001B[0m         gotit \u001B[38;5;241m=\u001B[39m waiter\u001B[38;5;241m.\u001B[39macquire(\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "#read_paths(network=fresno_network, update_incidence_matrices=True, filename='paths-fresno.csv')\n",
    "read_paths(network=fresno_network, update_incidence_matrices=True, filename = 'paths-full-model-fresno.csv')\n",
    "\n",
    "# For quick testing\n",
    "# Q = fresno_network.load_OD(sparsify_OD(fresno_network.Q, prop_od_pairs=0.99))\n",
    "# load_k_shortest_paths(network=fresno_network, k=2, update_incidence_matrices=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read spatiotemporal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folderpath = isl.config.dirs['read_network_data'] + 'links/spatiotemporal-data/'\n",
    "df = pd.concat([pd.read_csv(file) for file in glob.glob(folderpath + \"*link-data*\")], axis=0)\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')\n",
    "# Select data from Tuesday to Thursday\n",
    "df = df[df['date'].dt.dayofweek.between(1, 3)]\n",
    "# df = df[df['date'].dt.year == 2019]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add period id for timevarying estimation\n",
    "\n",
    "period_feature = 'hour'\n",
    "\n",
    "df['period'] = df['date'].astype(str) + '-' + df[period_feature].astype(str)\n",
    "# df['period'] = df.period.map(hash)\n",
    "\n",
    "df = add_period_id(df, period_feature='hour')\n",
    "\n",
    "period_keys = df[[period_feature,'period_id']].drop_duplicates().reset_index().drop('index',axis =1).sort_values('hour')\n",
    "print(period_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Units in miles per hour\n",
    "df[['speed_ref_avg','speed_hist_avg','speed_max']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tt_ff'] = np.where(df['link_type'] != 'LWRLK', 0,df['length']/df['speed_ref_avg'])\n",
    "df.loc[(df.link_type == \"LWRLK\") & (df.speed_ref_avg == 0),'tt_ff'] = float('nan')\n",
    "\n",
    "df['tt_avg'] = np.where(df['link_type'] != 'LWRLK', 0,df['length']/df['speed_hist_avg'])\n",
    "df.loc[(df.link_type == \"LWRLK\") & (df.speed_hist_avg == 0),'tt_avg'] = float('nan')\n",
    "\n",
    "tt_sd_adj = df.groupby(['period_id','link_key'])[['tt_avg']].std().reset_index().rename(columns = {'tt_avg': 'tt_sd_adj'})\n",
    "\n",
    "df = df.merge(tt_sd_adj, on = ['period_id','link_key'])\n",
    "\n",
    "df = data_curation(df)\n",
    "\n",
    "df['tt_sd'] = df['tt_sd_adj']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Units of travel time features are converted from hours to minutes\n",
    "df['tt_sd'] = df['tt_sd']*60\n",
    "df['tt_avg'] = df['tt_avg']*60\n",
    "df['tt_ff'] = df['tt_ff']*60"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[['speed_ref_avg','speed_hist_avg', 'tt_ff', 'tt_avg','tt_sd_adj']].describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_Z = ['tt_sd', 'median_inc', 'incidents', 'bus_stops', 'intersections']\n",
    "# features_Z = ['speed_sd', 'median_inc', 'incidents', 'bus_stops', 'intersections']\n",
    "\n",
    "\n",
    "utility_parameters = UtilityParameters(features_Y=['tt'],\n",
    "                                       features_Z=features_Z,\n",
    "                                       initial_values={'tt': 0, 'c': 0, 's': 0, 'psc_factor': 0,\n",
    "                                                       'fixed_effect': np.zeros_like(fresno_network.links)},\n",
    "                                       signs={'tt': '-', 'tt_sd': '-', 'median_inc': '+', 'incidents': '-',\n",
    "                                              'bus_stops': '-', 'intersections': '-'},\n",
    "                                       trainables={'psc_factor': False, 'fixed_effect': False},\n",
    "                                       )\n",
    "\n",
    "utility_parameters.constant_initializer(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_links = len(fresno_network.links)\n",
    "df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')\n",
    "df['year'] = df.date.dt.year\n",
    "X, Y = {}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query('year == 2019')[['counts', 'tt_ff', 'tt_avg', 'tf_inrix', 'tt_sd']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query('year == 2020')[['counts', 'tt_ff', 'tt_avg', 'tf_inrix', 'tt_sd']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set free flow travel times\n",
    "tt_ff_links = df.groupby('link_key')['tt_ff'].min()\n",
    "for link in fresno_network.links:\n",
    "    fresno_network.links_dict[link.key].performance_function.tf = float(tt_ff_links[tt_ff_links.index==str(link.key)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This correlation should be positive\n",
    "df[['counts','tt_avg']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check that there is a balanced amount of observations per date\n",
    "obs_date = df.groupby('date')['hour'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats by date\n",
    "df.groupby('date')[['speed_sd','speed_avg', 'counts']].mean().assign(total_obs = obs_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Link attributes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[features_Z].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include only data between 4pm and 5pm\n",
    "# X, Y = get_tensors_by_year(df, features_Z = features_Z, network = fresno_network)\n",
    "#X, Y = get_tensors_by_year(df[df.hour.isin([7])], features_Z = features_Z, network = fresno_network)\n",
    "X, Y = get_tensors_by_year(df[df.hour.isin([16])], features_Z = features_Z, network = fresno_network)\n",
    "# X, Y = get_tensors_by_year(df[df.hour.isin([15,16,17])], features_Z = features_Z, network = fresno_network)\n",
    "#X, Y = get_tensors_by_year(df[df.hour.isin([6,7,8])], features_Z = features_Z, network = fresno_network)\n",
    "\n",
    "# Include hourly data between 6AM and 8PM (15 hour intervals)\n",
    "# XT, YT = get_tensors_by_year(df, features_Z = features_Z)\n",
    "#XT, YT = get_tensors_by_year(df[df.hour.isin(range(15,16))], features_Z = features_Z, network = fresno_network)\n",
    "XT, YT = get_tensors_by_year(df[df.hour.isin([6,7,8, 15,16,17])], features_Z = features_Z, network = fresno_network)\n",
    "#XT, YT = get_tensors_by_year(df[df.hour.isin([6,7,8])], features_Z = features_Z, network = fresno_network)\n",
    "\n",
    "# X_train, X_test, Y_train, Y_test = temporal_split(X[2019].numpy(), Y[2019].numpy(), n_days = X[2019].shape[0])\n",
    "\n",
    "# X_train, X_test, Y_train, Y_test = X[2020], X[2019], Y[2020], Y[2019]\n",
    "X_train, X_test, Y_train, Y_test = X[2019], X[2020], Y[2019], Y[2020]\n",
    "XT_train, XT_test, YT_train, YT_test = XT[2019], XT[2020], YT[2019], YT[2020]\n",
    "\n",
    "# Remove validation set to reduce computation costs\n",
    "X_test, Y_test = None, None\n",
    "XT_test, YT_test = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network equilibrium predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equilibrator = Equilibrator(\n",
    "    network=fresno_network,\n",
    "    # paths_generator=paths_generator,\n",
    "    utility=utility_parameters,\n",
    "    max_iters=100,\n",
    "    method='fw',\n",
    "    iters_fw=50,\n",
    "    accuracy=1e-4,\n",
    ")\n",
    "\n",
    "column_generator = ColumnGenerator(equilibrator=equilibrator,\n",
    "                                   utility=utility_parameters,\n",
    "                                   n_paths=0,\n",
    "                                   ods_coverage=0.1,\n",
    "                                   ods_sampling='sequential',\n",
    "                                   # ods_sampling='demand',\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing\n",
    "# _EPOCHS = {'learning': 4, 'equilibrium': 2}\n",
    "# _EPOCHS = {'learning': 10, 'equilibrium': 5}\n",
    "_EPOCHS = {'learning': 150, 'equilibrium': 100}\n",
    "_BATCH_SIZE = 16\n",
    "_LR = 5e-1\n",
    "_RELATIVE_GAP = 1e-6\n",
    "_XTICKS_SPACING = 50\n",
    "_EPOCHS_PRINT_INTERVAL = 1\n",
    "\n",
    "# When adding fixed effects, all parameters of the utility function are idenfiable but provided that the utility coefficients are period specific. The\n",
    "# only attribute that is always identifiable is the standard deviation of travel time because it changes over hours for same link. Overall, removing fixed effect ease the identification of the parameters of the utility function.\n",
    "_FIXED_EFFECT = False\n",
    "# features_Z = ['tt_sd']\n",
    "\n",
    "# Excluding historic OD gives more freedom for the model to find an equilibria and minimize reconstruction error\n",
    "# _LOSS_WEIGHTS ={'od': 0, 'tt': 1, 'flow': 1, 'eq_flow': 1, 'ntrips': 1, 'prop_od': 1}\n",
    "_LOSS_WEIGHTS ={'od': 0, 'tt': 1, 'flow': 1, 'eq_flow': 1, 'ntrips': 0, 'prop_od': 0}\n",
    "\n",
    "# _LOSS_METRIC  = mnrmse\n",
    "# _MOMENTUM_EQUILIBRIUM = {'lue': 0.95, 'odlue': 0.95, 'odlulpe': 0.95, 'tvodlulpe':0.95}\n",
    "\n",
    "# NRMSE encourages a larger reduction in link flow loss and it does not requires to add much momentum to the equilibrium loss component\n",
    "_LOSS_METRIC  = nrmse\n",
    "_MOMENTUM_EQUILIBRIUM = {'lue': 0.99, 'odlue': 0.99, 'odlulpe': 0.99, 'tvodlulpe':0.99}\n",
    "# _MOMENTUM_EQUILIBRIUM = {'lue': 0.95, 'odlue': 0.95, 'odlulpe': 0.95, 'tvodlulpe':0.95}\n",
    "#_MOMENTUM_EQUILIBRIUM = {'lue': 0.8, 'odlue': 0.8, 'odlulpe': 0.8, 'tvodlulpe':0.8}\n",
    "\n",
    "# Including historic OD matrix\n",
    "# _LOSS_WEIGHTS ={'od': 1, 'tt': 1, 'flow': 1, 'eq_flow': 1}\n",
    "# _MOMENTUM_EQUILIBRIUM = 0.99\n",
    "\n",
    "# _LOSS_METRIC = mse\n",
    "# _LOSS_WEIGHTS ={'od': 1, 'theta': 0, 'tt': 1e10, 'flow': 1, 'eq_flow': 1}\n",
    "\n",
    "print(f\"_LOSS_WEIGHTS: {_LOSS_WEIGHTS}, _MOMENTUM_EQUILIBRIUM: {_MOMENTUM_EQUILIBRIUM}, \"\n",
    "      f\"epochs: { _EPOCHS}, 'LR: {_LR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model = dict.fromkeys(['lue', 'odlue','odlulpe', 'tvodlulpe'], True)\n",
    "# run_model = dict.fromkeys(['lue', 'odlue','odlulpe', 'tvodlulpe'], False)\n",
    "\n",
    "# run_model['lue'] = True\n",
    "# run_model['odlue'] = True\n",
    "# run_model['odlulpe'] = True\n",
    "# run_model['tvodlulpe'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_results_dfs = {}\n",
    "test_results_dfs = {}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model 1: Estimation of utility function (LUE)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if run_model['lue']:\n",
    "    print('\\nLUE: Estimation of utility function')\n",
    "\n",
    "    # _MOMENTUM_EQUILIBRIUM = 1\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=_LR)\n",
    "\n",
    "    _FIXED_EFFECT = False\n",
    "\n",
    "    utility_parameters = UtilityParameters(features_Y=['tt'],\n",
    "                                           features_Z=features_Z,\n",
    "                                           # initial_values ={'tt': -1, 'tt_sd': -1,\n",
    "                                           #                  'median_inc': 1,\n",
    "                                           #                  'incidents': -1, 'bus_stops': -1, 'intersections': -1,\n",
    "                                           #                  'psc_factor': 0, 'fixed_effect': np.zeros_like(fresno_network.links)},\n",
    "                                           initial_values={'psc_factor': 0,\n",
    "                                                           'fixed_effect': np.zeros_like(fresno_network.links)},\n",
    "                                           signs={'tt': '-', 'tt_sd': '-', 'median_inc': '+', 'incidents': '-',\n",
    "                                                  'bus_stops': '-', 'intersections': '-'},\n",
    "                                           trainables={'psc_factor': False, 'fixed_effect': _FIXED_EFFECT,\n",
    "                                                       'tt': True, 'tt_sd': True,\n",
    "                                                       # 'median_inc': False, 'incidents': False,\n",
    "                                                       # 'bus_stops': False, 'intersections': False,\n",
    "                                                       'median_inc': True, 'incidents': True,\n",
    "                                                       'bus_stops': True, 'intersections': True\n",
    "                                                       },\n",
    "                                           time_varying = True\n",
    "                                           )\n",
    "\n",
    "    bpr_parameters = BPRParameters(keys=['alpha', 'beta'],\n",
    "                                   initial_values={'alpha': 0.15, 'beta': 4},\n",
    "                                   trainables=dict.fromkeys(['alpha', 'beta'], False),\n",
    "                                   )\n",
    "\n",
    "    od_parameters = ODParameters(key='od',\n",
    "                                 initial_values=fresno_network.q.flatten(),\n",
    "                                 true_values=fresno_network.q.flatten(),\n",
    "                                 historic_values={10: fresno_network.q.flatten()},\n",
    "                                 trainable=False)\n",
    "\n",
    "    equilibrator = Equilibrator(\n",
    "        network=fresno_network,\n",
    "        utility=utility_parameters,\n",
    "        max_iters=100,\n",
    "        method='fw',\n",
    "        iters_fw=50,\n",
    "        accuracy=1e-4,\n",
    "    )\n",
    "\n",
    "    column_generator = ColumnGenerator(equilibrator=equilibrator,\n",
    "                                       utility=utility_parameters,\n",
    "                                       n_paths=0,\n",
    "                                       ods_coverage=0.1,\n",
    "                                       ods_sampling='sequential',\n",
    "                                       )\n",
    "\n",
    "    lue = GISUELOGIT(\n",
    "        key='lue',\n",
    "        network=fresno_network,\n",
    "        dtype=tf.float64,\n",
    "        equilibrator = equilibrator,\n",
    "        utility=utility_parameters,\n",
    "        bpr=bpr_parameters,\n",
    "        od=od_parameters,\n",
    "        n_periods = len(np.unique(X_train[:,:,-1].numpy().flatten()))\n",
    "    )\n",
    "\n",
    "    train_results_dfs['lue'], test_results_dfs['lue'] = lue.train(\n",
    "        X_train, Y_train, X_test, Y_test,\n",
    "        optimizer=optimizer,\n",
    "        batch_size=_BATCH_SIZE,\n",
    "        threshold_relative_gap=_RELATIVE_GAP,\n",
    "        momentum_equilibrium = _MOMENTUM_EQUILIBRIUM['lue'],\n",
    "        loss_weights= dict(_LOSS_WEIGHTS, od = 0),\n",
    "        loss_metric=_LOSS_METRIC,\n",
    "        epochs_print_interval=_EPOCHS_PRINT_INTERVAL,\n",
    "        epochs=_EPOCHS)\n",
    "\n",
    "    # Relative loss curves over epochs\n",
    "    plot_predictive_performance(train_losses=train_results_dfs['lue'], val_losses=test_results_dfs['lue'],\n",
    "                                xticks_spacing = _XTICKS_SPACING)\n",
    "\n",
    "    # Compute utility parameters over time (heatmap) and value of travel time reliability (lineplot)\n",
    "    theta_df = plot_utility_parameters_periods(lue, period_keys = period_keys, period_feature='hour')\n",
    "\n",
    "    plot_rr_by_period(lue, period_keys, period_feature='hour')\n",
    "\n",
    "    # Average reliability ratio over epochs\n",
    "    plot_convergence_estimates(estimates=train_results_dfs['lue'].\\\n",
    "                       assign(rr = train_results_dfs['lue']['tt_sd']/train_results_dfs['lue']['tt'])[['epoch','rr']],\n",
    "                           xticks_spacing = _XTICKS_SPACING)\n",
    "\n",
    "    plt.ylabel('average reliability ratio')\n",
    "\n",
    "    # Distribution of fixed effects\n",
    "    sns.displot(pd.DataFrame({'fixed_effect':np.array(lue.fixed_effect)}),\n",
    "        x=\"fixed_effect\", multiple=\"stack\", kind=\"kde\", alpha = 0.8)\n",
    "\n",
    "    #print(f\"theta = {dict(zip(utility_parameters.true_values.keys(), list(lue.theta.numpy())))}\")\n",
    "    print(f\"theta:\\n\\n\\ {theta_df.assign(rr = theta_df['tt_sd']/theta_df['tt'])}\")\n",
    "    print(f\"alpha = {lue.alpha: 0.2f}, beta  = {lue.beta: 0.2f}\")\n",
    "    print(f\"Avg abs diff of observed and estimated OD: {np.mean(np.abs(lue.q - fresno_network.q.flatten())): 0.2f}\")\n",
    "    print(f\"Avg observed OD: {np.mean(np.abs(fresno_network.q.flatten())): 0.2f}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model 2: OD + utility estimation with historic OD (ODLUE)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if run_model['odlue']:\n",
    "\n",
    "    # _MOMENTUM_EQUILIBRIUM = 0.99\n",
    "\n",
    "    print('\\nODLUE: OD + utility estimation with historic OD')\n",
    "\n",
    "    # _RELATIVE_GAP = 1e-4\\\n",
    "    # _XTICKS_SPACING = 50\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=_LR)\n",
    "    # optimizer = tf.keras.optimizers.Adagrad(learning_rate=_LR)\n",
    "    _FIXED_EFFECT = False\n",
    "\n",
    "    utility_parameters = UtilityParameters(features_Y=['tt'],\n",
    "                                           features_Z=features_Z,\n",
    "                                           # initial_values ={'tt': -1, 'tt_sd': -1,\n",
    "                                           #                  'median_inc': 1,\n",
    "                                           #                  'incidents': -1, 'bus_stops': -1, 'intersections': -1,\n",
    "                                           #                  'psc_factor': 0, 'fixed_effect': np.zeros_like(fresno_network.links)},\n",
    "                                           initial_values={'psc_factor': 0,\n",
    "                                                           'fixed_effect': np.zeros_like(fresno_network.links)},\n",
    "                                           signs={'tt': '-', 'tt_sd': '-', 'median_inc': '+', 'incidents': '-',\n",
    "                                                  'bus_stops': '-', 'intersections': '-'},\n",
    "                                           trainables={'psc_factor': False, 'fixed_effect': _FIXED_EFFECT,\n",
    "                                                       'tt': True, 'tt_sd': True,\n",
    "                                                       # 'median_inc': False, 'incidents': False,\n",
    "                                                       # 'bus_stops': False, 'intersections': False,\n",
    "                                                       'median_inc': True, 'incidents': True,\n",
    "                                                       'bus_stops': True, 'intersections': True\n",
    "                                                       },\n",
    "                                           time_varying = True\n",
    "                                           )\n",
    "\n",
    "    bpr_parameters = BPRParameters(keys=['alpha', 'beta'],\n",
    "                                   initial_values={'alpha': 0.15, 'beta': 4},\n",
    "                                   trainables=dict.fromkeys(['alpha', 'beta'], False),\n",
    "                                   )\n",
    "\n",
    "    od_parameters = ODParameters(key='od',\n",
    "                                 initial_values=fresno_network.q.flatten(),\n",
    "                                 historic_values={10: fresno_network.q.flatten()},\n",
    "                                 # total_trips={10: 1e5},\n",
    "                                 trainable=True)\n",
    "\n",
    "    equilibrator = Equilibrator(\n",
    "        network=fresno_network,\n",
    "        # paths_generator=paths_generator,\n",
    "        utility=utility_parameters,\n",
    "        max_iters=100,\n",
    "        method='fw',\n",
    "        iters_fw=50,\n",
    "        accuracy=1e-4,\n",
    "    )\n",
    "\n",
    "    column_generator = ColumnGenerator(equilibrator=equilibrator,\n",
    "                                       utility=utility_parameters,\n",
    "                                       n_paths=0,\n",
    "                                       ods_coverage=0.1,\n",
    "                                       ods_sampling='sequential',\n",
    "                                       # ods_sampling='demand',\n",
    "                                       )\n",
    "\n",
    "    odlue = GISUELOGIT(\n",
    "        key='odlue',\n",
    "        network=fresno_network,\n",
    "        dtype=tf.float64,\n",
    "        equilibrator=equilibrator,\n",
    "        column_generator=column_generator,\n",
    "        utility=utility_parameters,\n",
    "        bpr=bpr_parameters,\n",
    "        od=od_parameters,\n",
    "        n_periods = len(np.unique(X_train[:,:,-1].numpy().flatten()))\n",
    "    )\n",
    "\n",
    "    train_results_dfs['odlue'], test_results_dfs['odlue'] = odlue.train(\n",
    "        X_train, Y_train, X_test, Y_test,\n",
    "        optimizer=optimizer,\n",
    "        batch_size=_BATCH_SIZE,\n",
    "        # generalization_error={'train': False, 'validation': True},\n",
    "        loss_weights= _LOSS_WEIGHTS,\n",
    "        loss_metric=_LOSS_METRIC,\n",
    "        momentum_equilibrium = _MOMENTUM_EQUILIBRIUM['odlue'],\n",
    "        threshold_relative_gap=_RELATIVE_GAP,\n",
    "        epochs_print_interval=_EPOCHS_PRINT_INTERVAL,\n",
    "        epochs=_EPOCHS)\n",
    "\n",
    "    plot_predictive_performance(train_losses=train_results_dfs['odlue'], val_losses=test_results_dfs['odlue'],\n",
    "                                xticks_spacing = _XTICKS_SPACING)\n",
    "\n",
    "    plot_top_od_flows_periods(odlue,\n",
    "                              historic_od= fresno_network.q.flatten(),\n",
    "                              period_keys = period_keys,\n",
    "                              period_feature='hour', top_k=20)\n",
    "\n",
    "    # Compute utility parameters over time (heatmap) and value of travel time reliability (lineplot)\n",
    "    theta_df = plot_utility_parameters_periods(odlue, period_keys = period_keys, period_feature='hour')\n",
    "\n",
    "    plot_rr_by_period(odlue, period_keys, period_feature='hour')\n",
    "\n",
    "    plot_convergence_estimates(estimates=train_results_dfs['odlue'].\\\n",
    "                       assign(rr = train_results_dfs['odlue']['tt_sd']/train_results_dfs['odlue']['tt'])[['epoch','rr']],\n",
    "                           xticks_spacing = _XTICKS_SPACING)\n",
    "    plt.ylabel('average reliability ratio')\n",
    "\n",
    "    sns.displot(pd.DataFrame({'fixed_effect':np.array(odlue.fixed_effect)}),\n",
    "            x=\"fixed_effect\", multiple=\"stack\", kind=\"kde\", alpha = 0.8)\n",
    "\n",
    "    #print(f\"theta = {dict(zip(utility_parameters.true_values.keys(), list(odlue.theta.numpy())))}\")\n",
    "    print(f\"theta:\\n\\n\\ {theta_df.assign(rr = theta_df['tt_sd']/theta_df['tt'])}\")\n",
    "    print(f\"alpha = {odlue.alpha: 0.2f}, beta  = {odlue.beta: 0.2f}\")\n",
    "    print(f\"Avg abs diff of observed and estimated OD: {np.mean(np.abs(odlue.q - fresno_network.q.flatten())): 0.2f}\")\n",
    "    print(f\"Avg observed OD: {np.mean(np.abs(fresno_network.q.flatten())): 0.2f}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model 3: ODLUE + link specific performance parameters (ODLULPE)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if run_model['odlulpe']:\n",
    "\n",
    "    print('\\nODLULPE: ODLUE + link performance parameters with historic OD matrix (link specifics alphas and betas)')\n",
    "\n",
    "    # _MOMENTUM_EQUILIBRIUM = 0.99\n",
    "    _FIXED_EFFECT = False\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=_LR)\n",
    "\n",
    "    # _LR = 5e-1\n",
    "    # _RELATIVE_GAP = 1e-5\n",
    "\n",
    "    # Some initializations of the bpr parameters, makes the optimization to fail (e.g. alpha =1, beta = 1). Using a common\n",
    "    # alpha but different betas for every link make the estimation more stable but there is overfitting after a certain amount of iterations\n",
    "\n",
    "    bpr_parameters = BPRParameters(keys=['alpha', 'beta'],\n",
    "                                   initial_values={'alpha': 0.15*np.ones_like(fresno_network.links,dtype = np.float32),\n",
    "                                                   'beta': 4*np.ones_like(fresno_network.links,dtype = np.float32),\n",
    "                                                   # 'beta': 4\n",
    "                                                   },\n",
    "                                   trainables={'alpha': True, 'beta': True},\n",
    "                                   # trainables={'alpha': True, 'beta': False},\n",
    "                                   )\n",
    "\n",
    "    od_parameters = ODParameters(key='od',\n",
    "                                 initial_values=fresno_network.q.flatten(),\n",
    "                                 historic_values={10: fresno_network.q.flatten()},\n",
    "                                 # total_trips={10: 1e5},\n",
    "                                 trainable=True\n",
    "                                 )\n",
    "\n",
    "    utility_parameters = UtilityParameters(features_Y=['tt'],\n",
    "                                           features_Z=features_Z,\n",
    "                                           # initial_values ={'tt': -1, 'tt_sd': -1,\n",
    "                                           #                  'median_inc': 1,\n",
    "                                           #                  'incidents': -1, 'bus_stops': -1, 'intersections': -1,\n",
    "                                           #                  'psc_factor': 0, 'fixed_effect': np.zeros_like(fresno_network.links)},\n",
    "                                           initial_values={'psc_factor': 0,\n",
    "                                                           'fixed_effect': np.zeros_like(fresno_network.links)},\n",
    "                                           signs={'tt': '-', 'tt_sd': '-',\n",
    "                                                  'median_inc': '+', 'incidents': '-',\n",
    "                                                  'bus_stops': '-', 'intersections': '-'\n",
    "                                                  },\n",
    "                                           trainables={'psc_factor': False, 'fixed_effect': _FIXED_EFFECT,\n",
    "                                                       'tt': True, 'tt_sd': True,\n",
    "                                                       # 'median_inc': False, 'incidents': False,\n",
    "                                                       # 'bus_stops': False, 'intersections': False,\n",
    "                                                       'median_inc': True, 'incidents': True,\n",
    "                                                       'bus_stops': True, 'intersections': True\n",
    "                                                       },\n",
    "                                           time_varying = True\n",
    "                                           )\n",
    "\n",
    "    equilibrator = Equilibrator(\n",
    "        network=fresno_network,\n",
    "        # paths_generator=paths_generator,\n",
    "        utility=utility_parameters,\n",
    "        max_iters=100,\n",
    "        method='fw',\n",
    "        iters_fw=50,\n",
    "        accuracy=1e-4,\n",
    "    )\n",
    "\n",
    "    column_generator = ColumnGenerator(equilibrator=equilibrator,\n",
    "                                       utility=utility_parameters,\n",
    "                                       n_paths=0,\n",
    "                                       ods_coverage=0.1,\n",
    "                                       ods_sampling='sequential',\n",
    "                                       # ods_sampling='demand',\n",
    "                                       )\n",
    "\n",
    "    odlulpe = GISUELOGIT(\n",
    "        key='odlulpe',\n",
    "        network=fresno_network,\n",
    "        dtype=tf.float64,\n",
    "        equilibrator=equilibrator,\n",
    "        column_generator=column_generator,\n",
    "        utility=utility_parameters,\n",
    "        bpr=bpr_parameters,\n",
    "        od=od_parameters,\n",
    "        n_periods = len(np.unique(X_train[:,:,-1].numpy().flatten()))\n",
    "    )\n",
    "\n",
    "    train_results_dfs['odlulpe'], test_results_dfs['odlulpe'] = odlulpe.train(\n",
    "        X_train, Y_train, X_test, Y_test,\n",
    "        optimizer=optimizer,\n",
    "        # generalization_error={'train': False, 'validation': True},\n",
    "        batch_size=_BATCH_SIZE,\n",
    "        # loss_weights={'od': 1, 'theta': 0, 'tt': 1, 'flow': 1, 'eq_flow': 1},\n",
    "        loss_weights=_LOSS_WEIGHTS,\n",
    "        momentum_equilibrium = _MOMENTUM_EQUILIBRIUM['odlulpe'],\n",
    "        threshold_relative_gap=_RELATIVE_GAP,\n",
    "        epochs_print_interval=_EPOCHS_PRINT_INTERVAL,\n",
    "        loss_metric=_LOSS_METRIC,\n",
    "        epochs=_EPOCHS)\n",
    "\n",
    "\n",
    "\n",
    "    plot_predictive_performance(train_losses=train_results_dfs['odlulpe'], val_losses=test_results_dfs['odlulpe'],\n",
    "                                xticks_spacing = _XTICKS_SPACING)\n",
    "\n",
    "    plot_convergence_estimates(estimates=train_results_dfs['odlulpe'][['epoch','alpha','beta']],\n",
    "                                xticks_spacing = _XTICKS_SPACING)\n",
    "\n",
    "    sns.displot(pd.melt(pd.DataFrame({'alpha':odlulpe.alpha, 'beta': odlulpe.beta}), var_name = 'parameters'),\n",
    "                x=\"value\", hue=\"parameters\", alpha = 0.8)\n",
    "\n",
    "    top_q, total_trips_by_hour = plot_top_od_flows_periods(odlulpe,\n",
    "                                                           historic_od= fresno_network.q.flatten(),\n",
    "                                                           period_keys = period_keys,\n",
    "                                                           period_feature='hour', top_k=20)\n",
    "\n",
    "    # Compute utility parameters over time (heatmap) and value of travel time reliability (lineplot)\n",
    "    theta_df = plot_utility_parameters_periods(odlulpe, period_keys = period_keys, period_feature='hour')\n",
    "\n",
    "    plot_rr_by_period(odlulpe, period_keys, period_feature='hour')\n",
    "\n",
    "\n",
    "    plot_convergence_estimates(estimates=train_results_dfs['odlulpe'].\\\n",
    "                   assign(rr = train_results_dfs['odlulpe']['tt_sd']/train_results_dfs['odlulpe']['tt'])[['epoch','rr']],\n",
    "                       xticks_spacing = _XTICKS_SPACING)\n",
    "\n",
    "    plt.ylabel('average reliability ratio')\n",
    "\n",
    "    sns.displot(pd.DataFrame({'fixed_effect':np.array(odlulpe.fixed_effect)}),\n",
    "                x=\"fixed_effect\", multiple=\"stack\", kind=\"kde\", alpha = 0.8)\n",
    "\n",
    "    #print(f\"theta = {dict(zip(utility_parameters.true_values.keys(), list(odlulpe.theta.numpy())))}\")\n",
    "    print(f\"theta:\\n\\n\\ {theta_df.assign(rr = theta_df['tt_sd']/theta_df['tt'])}\")\n",
    "    print(f\"alpha = {np.mean(odlulpe.alpha): 0.2f}, beta  = {np.mean(odlulpe.beta): 0.2f}\")\n",
    "    print(f\"Avg abs diff of observed and estimated OD: {np.mean(np.abs(odlulpe.q - fresno_network.q.flatten())): 0.2f}\")\n",
    "    print(f\"Avg observed OD: {np.mean(np.abs(fresno_network.q.flatten())): 0.2f}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model 4: ODLULPE with Time Varying OD and Utility Function (TVODLULPE)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if run_model['tvodlulpe']:\n",
    "    print('\\ntvodlulpe: Time specific utility and OD, link performance parameters')\n",
    "\n",
    "    _FIXED_EFFECT = True\n",
    "    _LR = 1e-1\n",
    "\n",
    "    # Only in this model, we can add fixed effect and have identifiable utility function coefficients\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=_LR)\n",
    "\n",
    "    utility_parameters = UtilityParameters(features_Y=['tt'],\n",
    "                                           features_Z=features_Z,\n",
    "                                           # initial_values ={'tt': -1, 'tt_sd': -1,\n",
    "                                           #                  'median_inc': 1,\n",
    "                                           #                  'incidents': -1, 'bus_stops': -1, 'intersections': -1,\n",
    "                                           #                  'psc_factor': 0, 'fixed_effect': np.zeros_like(fresno_network.links)},\n",
    "                                           initial_values={'tt': 0, 'tt_sd': 0, 's': 0, 'psc_factor': 0,\n",
    "                                                           'fixed_effect': np.zeros_like(fresno_network.links)},\n",
    "                                           signs={'tt': '-',\n",
    "                                                  'tt_sd': '-',\n",
    "                                                  'median_inc': '+', 'incidents': '-',\n",
    "                                                  'bus_stops': '-', 'intersections': '-'\n",
    "                                                  },\n",
    "                                           trainables={'psc_factor': False, 'fixed_effect': _FIXED_EFFECT,\n",
    "                                                       'tt': True, 'tt_sd': True,\n",
    "                                                       # 'median_inc': False, 'incidents': False,\n",
    "                                                       # 'bus_stops': False, 'intersections': False,\n",
    "                                                       'median_inc': True, 'incidents': True,\n",
    "                                                       'bus_stops': True, 'intersections': True\n",
    "                                                       },\n",
    "                                           time_varying=True,\n",
    "                                           )\n",
    "\n",
    "    bpr_parameters = BPRParameters(keys=['alpha', 'beta'],\n",
    "                                   initial_values={'alpha': 0.15*np.ones_like(fresno_network.links,dtype = np.float32),\n",
    "                                                   'beta': 4*np.ones_like(fresno_network.links,dtype = np.float32),\n",
    "                                                   # 'beta': 4\n",
    "                                                   },\n",
    "                                   trainables={'alpha': True, 'beta': True},\n",
    "                                   # trainables={'alpha': True, 'beta': False},\n",
    "                                   )\n",
    "\n",
    "    od_parameters = ODParameters(key='od',\n",
    "                                 initial_values=fresno_network.q.flatten(),\n",
    "                                 true_values=fresno_network.q.flatten(),\n",
    "                                 historic_values={10: fresno_network.q.flatten()},\n",
    "                                 # total_trips={0: 1e5, 1:1e5, 2: 1e5, 9: 1e5, 10: 1e5, 11: 1e5},\n",
    "                                 # total_trips={i:1e5 for i in [0,1,2,9,10,11]},\n",
    "                                 time_varying=True,\n",
    "                                 trainable=True)\n",
    "\n",
    "    tvodlulpe = GISUELOGIT(\n",
    "        key='tvodlulpe',\n",
    "        network=fresno_network,\n",
    "        dtype=tf.float64,\n",
    "        utility=utility_parameters,\n",
    "        bpr=bpr_parameters,\n",
    "        od=od_parameters,\n",
    "        n_periods = len(np.unique(XT_train[:,:,-1].numpy().flatten()))\n",
    "    )\n",
    "\n",
    "    train_results_dfs['tvodlulpe'], test_results_dfs['tvodlulpe'] = tvodlulpe.train(\n",
    "        XT_train, YT_train, XT_test, YT_test,\n",
    "        optimizer=optimizer,\n",
    "        # generalization_error={'train': False, 'validation': True},\n",
    "        batch_size=_BATCH_SIZE,\n",
    "        loss_weights= _LOSS_WEIGHTS,\n",
    "        loss_metric=_LOSS_METRIC,\n",
    "        momentum_equilibrium=_MOMENTUM_EQUILIBRIUM['tvodlulpe'],\n",
    "        threshold_relative_gap=_RELATIVE_GAP,\n",
    "        epochs=_EPOCHS)\n",
    "\n",
    "    plot_predictive_performance(train_losses=train_results_dfs['tvodlulpe'], val_losses=test_results_dfs['tvodlulpe'],\n",
    "                               xticks_spacing=_XTICKS_SPACING)\n",
    "\n",
    "    plot_convergence_estimates(estimates=train_results_dfs['tvodlulpe'][['epoch', 'alpha', 'beta']],\n",
    "                               xticks_spacing=_XTICKS_SPACING)\n",
    "\n",
    "    sns.displot(pd.melt(pd.DataFrame({'alpha':tvodlulpe.alpha, 'beta': tvodlulpe.beta}), var_name = 'parameters'),\n",
    "                x=\"value\", hue=\"parameters\", alpha = 0.8)\n",
    "\n",
    "    top_q, total_trips_by_hour = plot_top_od_flows_periods(tvodlulpe,\n",
    "                                                           period_keys = period_keys,\n",
    "                                                           historic_od= fresno_network.q.flatten(),\n",
    "                                                           period_feature='hour', top_k=20)\n",
    "\n",
    "    # Compute utility parameters over time (heatmap) and value of travel time reliability (lineplot)\n",
    "    theta_df = plot_utility_parameters_periods(tvodlulpe, period_keys = period_keys, period_feature='hour')\n",
    "\n",
    "    plot_rr_by_period(tvodlulpe, period_keys, period_feature='hour')\n",
    "\n",
    "    plot_convergence_estimates(estimates=train_results_dfs['tvodlulpe'].\\\n",
    "               assign(rr = train_results_dfs['tvodlulpe']['tt_sd']/train_results_dfs['tvodlulpe']['tt'])[['epoch','rr']],\n",
    "                   xticks_spacing = _XTICKS_SPACING)\n",
    "    plt.ylabel('average reliability ratio')\n",
    "\n",
    "    sns.displot(pd.DataFrame({'fixed_effect':np.array(tvodlulpe.fixed_effect)}),\n",
    "            x=\"fixed_effect\", multiple=\"stack\", kind=\"kde\", alpha = 0.8)\n",
    "\n",
    "    # print(f\"theta = {dict(zip(utility_parameters.true_values.keys(), list(tvodlulpe.theta.numpy())))}\")\n",
    "    print(f\"theta:\\n\\n\\ {theta_df.assign(rr = theta_df['tt_sd']/theta_df['tt'])}\")\n",
    "    print(f\"alpha = {np.mean(tvodlulpe.alpha): 0.2f}, beta  = {np.mean(tvodlulpe.beta): 0.2f}\")\n",
    "    print(f\"Avg abs diff of observed and estimated OD: {np.mean(np.abs(tvodlulpe.q - fresno_network.q.flatten())): 0.2f}\")\n",
    "    print(f\"Avg observed OD: {np.mean(np.abs(fresno_network.q.flatten())): 0.2f}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Write predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame({'link_key': list(fresno_network.links_keys) * Y_train.shape[0],\n",
    "                            'observed_traveltime': Y_train[:, :, 0].numpy().flatten(),\n",
    "                            'observed_flow': Y_train[:, :, 1].numpy().flatten()})\n",
    "\n",
    "predictions['date'] = sorted(df[df.hour == 16].loc[df[df.hour == 16].year == 2019, 'date'])\n",
    "\n",
    "# TODO: Write predictions for TVODLULPE model\n",
    "for model in [lue,odlue,odlulpe]:\n",
    "\n",
    "    predicted_flows = model.flows()\n",
    "    predicted_traveltimes = model.traveltimes()\n",
    "\n",
    "    predictions['predicted_traveltime_' + model.key] = np.tile(predicted_traveltimes, (Y_train.shape[0], 1)).flatten()\n",
    "    predictions['predicted_flow_' + model.key] = np.tile(predicted_flows, (Y_train.shape[0], 1)).flatten()\n",
    "\n",
    "predictions.to_csv(f\"./output/tables/{datetime.now().strftime('%y%m%d%H%M%S')}_train_predictions_{'Fresno'}.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write estimation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results_df, test_results_df \\\n",
    "    = map(lambda x: pd.concat([results.assign(model = model)[['model'] + list(results.columns)]\n",
    "                               for model, results in x.items()],axis = 0), [train_results_dfs, test_results_dfs])\n",
    "\n",
    "network_name = 'Fresno'\n",
    "\n",
    "train_filename = f\"{datetime.now().strftime('%y%m%d%H%M%S')}_train_results_{network_name}.csv\"\n",
    "test_filename = f\"{datetime.now().strftime('%y%m%d%H%M%S')}_validation_results_{network_name}.csv\"\n",
    "train_results_df.to_csv(f\"./output/tables/{train_filename}\")\n",
    "print(f'File {train_filename} was written')\n",
    "test_results_df.to_csv(f\"./output/tables/{test_filename}\")\n",
    "print(f'File {test_filename} was written')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of models parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [lue,odlue,odlulpe, tvodlulpe]\n",
    "results = pd.DataFrame({'parameter': [], 'model': []})\n",
    "\n",
    "for model in models:\n",
    "    results = results.append(pd.DataFrame(\n",
    "        {'parameter': ['tt'] + features_Z +\n",
    "                      ['rr'] +\n",
    "                      ['fixed_effect_mean', 'fixed_effect_std',\n",
    "                       'alpha_mean', 'alpha_std',\n",
    "                       'beta_mean', 'beta_std',\n",
    "                       'od_mean', 'od_std',],\n",
    "         'values': list(np.mean(model.theta.numpy(),axis =0))  +\n",
    "                   [float(model.get_parameters_estimates().eval('tt_sd/tt'))] +\n",
    "                   [np.mean(model.fixed_effect),np.std(model.fixed_effect),\n",
    "                    np.mean(model.alpha),np.std(model.alpha),\n",
    "                    np.mean(model.beta),np.std(model.beta),\n",
    "                    np.mean(model.q),np.std(model.q)]}).\\\n",
    "                             assign(model = model.key)\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.pivot_table(index = ['parameter'], columns = 'model', values = 'values', sort=False).round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of models goodness of fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [lue, odlue, odlulpe, tvodlulpe]\n",
    "results_losses = pd.DataFrame({})\n",
    "loss_columns = ['loss_flow', 'loss_tt', 'loss_eq_flow', 'loss_total']\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "\n",
    "    results_losses_model = model.split_results(train_results_dfs[model.key])[1].assign(model = model.key)\n",
    "    results_losses_model = results_losses_model[results_losses_model.epoch == results_losses_model.epoch.max()].iloc[[0]]\n",
    "    results_losses = results_losses.append(results_losses_model)\n",
    "\n",
    "results_losses[loss_columns] = (results_losses[loss_columns]-1)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results_losses[['model'] + list(results_losses.columns)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of convergence toward true rr across models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_estimates = {}\n",
    "train_losses = {}\n",
    "\n",
    "for model in models:\n",
    "    train_estimates[model.key], train_losses[model.key] = model.split_results(results=train_results_dfs[model.key])\n",
    "\n",
    "    train_estimates[model.key]['model'] = model.key\n",
    "\n",
    "train_estimates_df = pd.concat(train_estimates.values())\n",
    "\n",
    "train_estimates_df['rr'] = train_estimates_df['tt_sd']/train_estimates_df['tt']\n",
    "\n",
    "estimates = train_estimates_df[['epoch','model','rr']].reset_index().drop('index',axis = 1)\n",
    "estimates = estimates[estimates.epoch != 0]\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1)\n",
    "\n",
    "g = sns.lineplot(data=estimates, x='epoch', hue='model', y='rr')\n",
    "\n",
    "ax.set_ylabel('average reliability ratio')\n",
    "\n",
    "plt.ylim(ymin=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plot of reliability ratio by hour for all models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reliability_ratios = plot_rr_by_period_models(models, period_keys, period_feature='hour')\n",
    "reliability_ratios.groupby('model')[['rr']].mean().round(4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plot of total trips by hour for all models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "total_trips = plot_total_trips_models(models = models, period_feature = 'hour', period_keys = period_keys,\n",
    "                                      historic_od = fresno_network.q.flatten())\n",
    "total_trips.groupby('model')[['total_trips']].mean().round(1)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gisuelogit-qeH6w6zX-py3.9",
   "language": "python",
   "name": "gisuelogit-qeh6w6zx-py3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
