{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "# import cudf as pd\n",
    "import tensorflow as tf\n",
    "import isuelogit as isl\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main dir: /Users/pablo/Library/CloudStorage/OneDrive-Personal/data-science/github/gisuelogit\n"
     ]
    }
   ],
   "source": [
    "# Path management\n",
    "main_dir = str(Path(os.path.abspath('')).parents[1])\n",
    "os.chdir(main_dir)\n",
    "print('main dir:', main_dir)\n",
    "\n",
    "isl.config.dirs['read_network_data'] = \"input/network-data/fresno/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Internal modules\n",
    "from src.aesuelogit.models import UtilityParameters, BPRParameters, ODParameters, GISUELOGIT, NGD\n",
    "from src.aesuelogit.visualizations import plot_predictive_performance\n",
    "from src.aesuelogit.networks import load_k_shortest_paths, read_paths, build_fresno_network, \\\n",
    "    Equilibrator, sparsify_OD, ColumnGenerator, read_OD\n",
    "from src.aesuelogit.etl import get_design_tensor, get_y_tensor, data_curation, temporal_split\n",
    "from src.aesuelogit.descriptive_statistics import mse, btcg_mse, mnrmse, nrmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed for reproducibility\n",
    "_SEED = 2022\n",
    "np.random.seed(_SEED)\n",
    "random.seed(_SEED)\n",
    "tf.random.set_seed(_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Fresno network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fresno_network = build_fresno_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read OD matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Q (1789, 1789) read in 0.0[s] with sparse format\n",
      "66266.3 trips were loaded among 6970 o-d pairs\n"
     ]
    }
   ],
   "source": [
    "read_OD(network=fresno_network, sparse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18289 paths were read in 23.5[s]              \n",
      "\n",
      "18289 paths were loaded in the network\n",
      "\n",
      "Updating incidence matrices\n",
      "\n",
      "Matrix D (2413, 18289) generated in 26.7[s]               \n",
      "\n",
      "Matrix M (6970, 18289) generated in 12.1[s]               \n",
      "\n",
      "Matrix C (18289, 18289) generated in 4.9[s]               \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read_paths(network=fresno_network, update_incidence_matrices=True, filename='paths-fresno.csv')\n",
    "read_paths(network=fresno_network, update_incidence_matrices=True, filename = 'paths-full-model-fresno.csv')\n",
    "\n",
    "# For quick testing\n",
    "# Q = fresno_network.load_OD(sparsify_OD(fresno_network.Q, prop_od_pairs=0.99))\n",
    "# load_k_shortest_paths(network=fresno_network, k=2, update_incidence_matrices=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read spatiotemporal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "folderpath = isl.config.dirs['read_network_data'] + 'links/spatiotemporal-data/'\n",
    "df = pd.concat([pd.read_csv(file) for file in glob.glob(folderpath + \"*fresno-link-data*\")], axis=0)\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')\n",
    "df = df[df['date'].dt.dayofweek.between(0, 4)]\n",
    "# df = df[df['date'].dt.year == 2019]\n",
    "\n",
    "df['period'] = df['date'].astype(str) + '-' + df['hour'].astype(str)\n",
    "df['period'] = df.period.map(hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tt_ff'] = np.where(df['link_type'] != 'LWRLK', 0,df['length']/df['speed_ref_avg'])\n",
    "df.loc[(df.link_type == \"LWRLK\") & (df.speed_ref_avg == 0),'tt_ff'] = float('nan')\n",
    "\n",
    "df['tt_avg'] = np.where(df['link_type'] != 'LWRLK', 0,df['length']/df['speed_hist_avg'])\n",
    "df.loc[(df.link_type == \"LWRLK\") & (df.speed_hist_avg == 0),'tt_avg'] = float('nan')\n",
    "\n",
    "df = data_curation(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_Z = ['speed_sd', 'median_inc', 'incidents', 'bus_stops', 'intersections']\n",
    "\n",
    "utility_parameters = UtilityParameters(features_Y=['tt'],\n",
    "                                       features_Z=features_Z,\n",
    "                                       periods = 1,\n",
    "                                       initial_values={'tt': 0, 'c': 0, 's': 0, 'psc_factor': 0,\n",
    "                                                       'fixed_effect': np.zeros_like(fresno_network.links)},\n",
    "                                       signs={'tt': '-', 'speed_sd': '-', 'median_inc': '+', 'incidents': '-',\n",
    "                                              'bus_stops': '-', 'intersections': '-'},\n",
    "                                       trainables={'psc_factor': False, 'fixed_effect': False},\n",
    "                                       )\n",
    "\n",
    "utility_parameters.constant_initializer(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_links = len(fresno_network.links)\n",
    "df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')\n",
    "df['year'] = df.date.dt.year\n",
    "X, Y = {}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "            counts          tt_ff         tt_avg       tf_inrix\ncount  9704.000000  166497.000000  127290.000000  166497.000000\nmean   1880.469281       0.003388       0.003647       0.192740\nstd     791.369747       0.004391       0.004636       0.224399\nmin       3.000000       0.000000       0.000000       0.000000\n25%    1361.000000       0.000000       0.000000       0.000000\n50%    1787.000000       0.002470       0.002667       0.144000\n75%    2302.700000       0.004641       0.005429       0.272000\nmax    4807.000000       0.070339       0.059900       4.605000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>counts</th>\n      <th>tt_ff</th>\n      <th>tt_avg</th>\n      <th>tf_inrix</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>9704.000000</td>\n      <td>166497.000000</td>\n      <td>127290.000000</td>\n      <td>166497.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>1880.469281</td>\n      <td>0.003388</td>\n      <td>0.003647</td>\n      <td>0.192740</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>791.369747</td>\n      <td>0.004391</td>\n      <td>0.004636</td>\n      <td>0.224399</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>3.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>1361.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>1787.000000</td>\n      <td>0.002470</td>\n      <td>0.002667</td>\n      <td>0.144000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>2302.700000</td>\n      <td>0.004641</td>\n      <td>0.005429</td>\n      <td>0.272000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>4807.000000</td>\n      <td>0.070339</td>\n      <td>0.059900</td>\n      <td>4.605000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.query('year == 2019')[['counts', 'tt_ff', 'tt_avg', 'tf_inrix']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "            counts          tt_ff         tt_avg       tf_inrix\ncount  9290.000000  159258.000000  120497.000000  159258.000000\nmean   1822.742196       0.003351       0.003463       0.188521\nstd     795.855464       0.004363       0.004431       0.219121\nmin      31.000000       0.000000       0.000000       0.000000\n25%    1275.125000       0.000000       0.000000       0.000000\n50%    1715.000000       0.002429       0.002551       0.142000\n75%    2242.750000       0.004539       0.005131       0.263000\nmax    4798.000000       0.070339       0.058472       3.775000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>counts</th>\n      <th>tt_ff</th>\n      <th>tt_avg</th>\n      <th>tf_inrix</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>9290.000000</td>\n      <td>159258.000000</td>\n      <td>120497.000000</td>\n      <td>159258.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>1822.742196</td>\n      <td>0.003351</td>\n      <td>0.003463</td>\n      <td>0.188521</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>795.855464</td>\n      <td>0.004363</td>\n      <td>0.004431</td>\n      <td>0.219121</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>31.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>1275.125000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>1715.000000</td>\n      <td>0.002429</td>\n      <td>0.002551</td>\n      <td>0.142000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>2242.750000</td>\n      <td>0.004539</td>\n      <td>0.005131</td>\n      <td>0.263000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>4798.000000</td>\n      <td>0.070339</td>\n      <td>0.058472</td>\n      <td>3.775000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.query('year == 2020')[['counts', 'tt_ff', 'tt_avg', 'tf_inrix']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set free flow travel times\n",
    "tt_ff_links = df.groupby('link_key')['tt_ff'].min()\n",
    "for link in fresno_network.links:\n",
    "    fresno_network.links_dict[link.key].performance_function.tf = float(tt_ff_links[tt_ff_links.index==str(link.key)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-05 11:37:06.651821: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "df = df.sort_values(by = 'date').copy()\n",
    "\n",
    "for year in sorted(df['year'].unique()):\n",
    "    df_year = df[df['year'] == year]\n",
    "\n",
    "    n_days, n_hours = len(df_year.date.unique()), len(df_year.hour.unique())\n",
    "\n",
    "    traveltime_data = get_y_tensor(y=df_year[['tt_avg']], n_links=n_links, n_days=n_days, n_hours=n_hours)\n",
    "    flow_data = get_y_tensor(y=df_year[['counts']], n_links=n_links, n_days=n_days, n_hours=n_hours)\n",
    "\n",
    "    Y[year] = tf.concat([traveltime_data, flow_data], axis=3)\n",
    "\n",
    "    X[year] = get_design_tensor(Z=df_year[features_Z], n_links=n_links, n_days=n_days, n_hours=n_hours)\n",
    "\n",
    "    tt_ff = get_design_tensor(Z=df_year[['tt_ff']], n_links=n_links, n_days=n_days, n_hours=n_hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only pick data from one year\n",
    "X = X[2019]\n",
    "Y = Y[2019]\n",
    "\n",
    "# Prepare the training and validation dataset\n",
    "X, Y = tf.concat(X,axis = 0), tf.concat(Y,axis = 0)\n",
    "#X_train, X_test, Y_train, Y_test = train_test_split(X.numpy(), Y.numpy(), test_size=0.5, random_state=_SEED)\n",
    "X_train, X_test, Y_train, Y_test = temporal_split(X.numpy(), Y.numpy(), n_days = 20)\n",
    "X_train, X_test, Y_train, Y_test = [tf.constant(i) for i in [X_train, X_test, Y_train, Y_test]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network equilibrium predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "equilibrator = Equilibrator(\n",
    "    network=fresno_network,\n",
    "    # paths_generator=paths_generator,\n",
    "    utility=utility_parameters,\n",
    "    max_iters=100,\n",
    "    method='fw',\n",
    "    iters_fw=50,\n",
    "    accuracy=1e-4,\n",
    ")\n",
    "\n",
    "column_generator = ColumnGenerator(equilibrator=equilibrator,\n",
    "                                   utility=utility_parameters,\n",
    "                                   n_paths=0,\n",
    "                                   ods_coverage=0.1,\n",
    "                                   ods_sampling='sequential',\n",
    "                                   # ods_sampling='demand',\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "_EPOCHS = 1000\n",
    "_BATCH_SIZE = 16\n",
    "_LR = 5e-1\n",
    "_RELATIVE_GAP = 1e-4\n",
    "_XTICKS_SPACING = 50\n",
    "_EPOCHS_PRINT_INTERVAL = 10\n",
    "\n",
    "# _LOSS_METRIC = mse\n",
    "# _LOSS_WEIGHTS ={'od': 1, 'theta': 0, 'tt': 1e10, 'flow': 1, 'eq_flow': 1}\n",
    "\n",
    "#_LOSS_METRIC  = btcg_mse\n",
    "#_LOSS_METRIC  = mnrmse\n",
    "_LOSS_METRIC  = nrmse\n",
    "_LOSS_WEIGHTS ={'od': 1, 'theta': 0, 'tt': 1, 'flow': 1, 'eq_flow': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model = dict.fromkeys(['equilibrium', 'lue', 'ode', 'odlue', 'odlulpe-1','odlulpe-2', 'tvodlulpe'], False)\n",
    "\n",
    "run_model['ode'] = True\n",
    "run_model['lue'] = True\n",
    "run_model['odlue'] = True\n",
    "run_model['odlulpe-1'] = True\n",
    "run_model['odlulpe-2'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results_dfs = {}\n",
    "test_results_dfs = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Benchmark of aesuelogit and isuelogit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LUE: Benchmark of aesuelogit and isuelogit (utility only)\n",
      "\n",
      "Epoch: 0, n_train: 10, n_test: 10\n",
      "\n",
      "0: train_loss=2.3e+06, val_loss=2.3e+06, train_loss tt=3.8e-05, val_loss tt=3.6e-05, train_loss flow=2.3e+06, val_loss flow=2.3e+06, theta = [0. 0. 0. 0. 0. 0.], vot = nan, psc_factor = 0.0, avg abs theta fixed effect = 0, avg alpha=0.15, avg beta=4, loss demand=3.2e-29, relative x=1.4e-16, relative gap=1e+10, train tt equilibrium loss=4e-36, train flow equilibrium loss=3.1e-26, time:  15.4\n",
      "\n",
      "Epoch: 10, n_train: 10, n_test: 10\n",
      "\n",
      "10: train_loss=2.5e+06, val_loss=2.5e+06, train_loss tt=2.8e-05, val_loss tt=2.6e-05, train_loss flow=2.3e+06, val_loss flow=2.4e+06, theta = [-2.0818 -2.1783  0.     -2.2122 -2.0712 -2.1225], vot = nan, psc_factor = 0.0, avg abs theta fixed effect = 1, avg alpha=0.15, avg beta=4, loss demand=3.2e-29, relative x=0.28, relative gap=0.048, train tt equilibrium loss=2.5e-06, train flow equilibrium loss=1.2e+05, time:  179.9\n"
     ]
    }
   ],
   "source": [
    "if run_model['lue']:\n",
    "    print('\\nLUE: Benchmark of aesuelogit and isuelogit (utility only)')\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=_LR)\n",
    "\n",
    "    utility_parameters = UtilityParameters(features_Y=['tt'],\n",
    "                                           features_Z=features_Z,\n",
    "                                           periods=1,\n",
    "                                           initial_values={'psc_factor': 0,\n",
    "                                                           'fixed_effect': np.zeros_like(fresno_network.links)},\n",
    "                                           signs={'tt': '-', 'speed_sd': '-', 'median_inc': '+', 'incidents': '-',\n",
    "                                                  'bus_stops': '-', 'intersections': '-'},\n",
    "                                           trainables={'psc_factor': False, 'fixed_effect': True,\n",
    "                                                       'tt': True, 'speed_sd': True, 'median_inc': True, 'incidents': True,\n",
    "                                                              'bus_stops': True, 'intersections': True\n",
    "                                                       },\n",
    "                                           )\n",
    "\n",
    "    utility_parameters.constant_initializer(0)\n",
    "\n",
    "    bpr_parameters = BPRParameters(keys=['alpha', 'beta'],\n",
    "                                   initial_values={'alpha': 0.15, 'beta': 4},\n",
    "                                   trainables=dict.fromkeys(['alpha', 'beta'], False),\n",
    "                                   )\n",
    "\n",
    "    od_parameters = ODParameters(key='od',\n",
    "                                 periods=1,\n",
    "                                 initial_values=fresno_network.q.flatten(),\n",
    "                                 true_values=fresno_network.q.flatten(),\n",
    "                                 historic_values={1: fresno_network.q.flatten()},\n",
    "                                 trainable=False)\n",
    "\n",
    "    equilibrator = Equilibrator(\n",
    "        network=fresno_network,\n",
    "        utility=utility_parameters,\n",
    "        max_iters=100,\n",
    "        method='fw',\n",
    "        iters_fw=50,\n",
    "        accuracy=1e-4,\n",
    "    )\n",
    "\n",
    "    column_generator = ColumnGenerator(equilibrator=equilibrator,\n",
    "                                       utility=utility_parameters,\n",
    "                                       n_paths=0,\n",
    "                                       ods_coverage=0.1,\n",
    "                                       ods_sampling='sequential',\n",
    "                                       )\n",
    "\n",
    "    lue = GISUELOGIT(\n",
    "        key='lue',\n",
    "        network=fresno_network,\n",
    "        dtype=tf.float64,\n",
    "        equilibrator = equilibrator,\n",
    "        utility=utility_parameters,\n",
    "        bpr=bpr_parameters,\n",
    "        od=od_parameters\n",
    "    )\n",
    "\n",
    "    train_results_dfs['lue'], test_results_dfs['lue'] = lue.train(\n",
    "        X_train, Y_train, X_test, Y_test,\n",
    "        optimizer=optimizer,\n",
    "        batch_size=_BATCH_SIZE,\n",
    "        threshold_relative_gap=_RELATIVE_GAP,\n",
    "        loss_weights= dict(_LOSS_WEIGHTS, od = 0),\n",
    "        loss_metric=_LOSS_METRIC,\n",
    "        epochs_print_interval=_EPOCHS_PRINT_INTERVAL,\n",
    "        epochs=_EPOCHS)\n",
    "\n",
    "    plot_predictive_performance(train_losses=train_results_dfs['lue'], val_losses=test_results_dfs['lue'],\n",
    "                                xticks_spacing = _XTICKS_SPACING)\n",
    "\n",
    "\n",
    "    print(f\"theta = {dict(zip(utility_parameters.true_values.keys(), list(lue.theta.numpy())))}\")\n",
    "    print(f\"alpha = {lue.alpha: 0.2f}, beta  = {lue.beta: 0.2f}\")\n",
    "    print(f\"Avg abs diff of observed and estimated OD: {np.mean(np.abs(lue.q - fresno_network.q.flatten())): 0.2f}\")\n",
    "    print(f\"Avg observed OD: {np.mean(np.abs(fresno_network.q.flatten())): 0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: OD + utility estimation with historic OD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ODLUE: OD + utility estimation with historic OD\n",
      "\n",
      "Epoch: 0, n_train: 10, n_test: 10\n",
      "\n",
      "0: train_loss=2.5, val_loss=37, train_loss tt=0.4, val_loss tt=0.37, train_loss flow=2.1, val_loss flow=36, theta = [0. 0. 0. 0. 0. 0.], vot = nan, psc_factor = 0.0, avg abs theta fixed effect = 0, avg alpha=0.15, avg beta=4, loss demand=8.6e-33, relative x=1.4e-16, relative gap=1e+10, train tt equilibrium loss=0.14, train flow equilibrium loss=0.0075, time:  13.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pablo/Library/CloudStorage/OneDrive-Personal/data-science/github/gisuelogit/src/aesuelogit/models.py:997: RuntimeWarning: Mean of empty slice\n",
      "  relative_x = float(np.nanmean(np.abs(1 * (tf.divide(link_flow,self.flows()) - 1))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some elements in the path flow vector are negative\n",
      "some elements in the path flow vector are negative\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [34]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     45\u001B[0m column_generator \u001B[38;5;241m=\u001B[39m ColumnGenerator(equilibrator\u001B[38;5;241m=\u001B[39mequilibrator,\n\u001B[1;32m     46\u001B[0m                                    utility\u001B[38;5;241m=\u001B[39mutility_parameters,\n\u001B[1;32m     47\u001B[0m                                    n_paths\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     50\u001B[0m                                    \u001B[38;5;66;03m# ods_sampling='demand',\u001B[39;00m\n\u001B[1;32m     51\u001B[0m                                    )\n\u001B[1;32m     53\u001B[0m odlue \u001B[38;5;241m=\u001B[39m GISUELOGIT(\n\u001B[1;32m     54\u001B[0m     key\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124modlue\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     55\u001B[0m     network\u001B[38;5;241m=\u001B[39mfresno_network,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     61\u001B[0m     od\u001B[38;5;241m=\u001B[39mod_parameters,\n\u001B[1;32m     62\u001B[0m )\n\u001B[0;32m---> 64\u001B[0m train_results_dfs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124modlue\u001B[39m\u001B[38;5;124m'\u001B[39m], test_results_dfs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124modlue\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43modlue\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     65\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY_test\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     66\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     67\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_BATCH_SIZE\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     68\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# generalization_error={'train': False, 'validation': True},\u001B[39;49;00m\n\u001B[1;32m     69\u001B[0m \u001B[43m    \u001B[49m\u001B[43mloss_weights\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m_LOSS_WEIGHTS\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     70\u001B[0m \u001B[43m    \u001B[49m\u001B[43mloss_metric\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_LOSS_METRIC\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     71\u001B[0m \u001B[43m    \u001B[49m\u001B[43mthreshold_relative_gap\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_RELATIVE_GAP\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     72\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs_print_interval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_EPOCHS_PRINT_INTERVAL\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     73\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_EPOCHS\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     75\u001B[0m plot_predictive_performance(train_losses\u001B[38;5;241m=\u001B[39mtrain_results_dfs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124modlue\u001B[39m\u001B[38;5;124m'\u001B[39m], val_losses\u001B[38;5;241m=\u001B[39mtest_results_dfs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124modlue\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m     76\u001B[0m                             xticks_spacing \u001B[38;5;241m=\u001B[39m _XTICKS_SPACING)\n\u001B[1;32m     78\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtheta = \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mdict\u001B[39m(\u001B[38;5;28mzip\u001B[39m(utility_parameters\u001B[38;5;241m.\u001B[39mtrue_values\u001B[38;5;241m.\u001B[39mkeys(), \u001B[38;5;28mlist\u001B[39m(odlue\u001B[38;5;241m.\u001B[39mtheta\u001B[38;5;241m.\u001B[39mnumpy())))\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Library/CloudStorage/OneDrive-Personal/data-science/github/gisuelogit/src/aesuelogit/models.py:1084\u001B[0m, in \u001B[0;36mGISUELOGIT.train\u001B[0;34m(self, X_train, Y_train, X_val, Y_val, optimizer, loss_weights, threshold_relative_gap, loss_metric, generalization_error, epochs, epochs_print_interval, batch_size)\u001B[0m\n\u001B[1;32m   1080\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m step, (X_batch_train, Y_batch_train) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(train_dataset):\n\u001B[1;32m   1082\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mGradientTape() \u001B[38;5;28;01mas\u001B[39;00m tape:\n\u001B[1;32m   1083\u001B[0m         train_loss \u001B[38;5;241m=\u001B[39m \\\n\u001B[0;32m-> 1084\u001B[0m             \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloss_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX_batch_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mY_batch_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlambdas\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcurrent_loss_weights\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1085\u001B[0m \u001B[43m                               \u001B[49m\u001B[43mloss_metric\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mloss_metric\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mloss_total\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m   1088\u001B[0m     grads \u001B[38;5;241m=\u001B[39m tape\u001B[38;5;241m.\u001B[39mgradient(train_loss, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainable_variables)\n\u001B[1;32m   1090\u001B[0m     \u001B[38;5;66;03m# # Apply some clipping (tf.linalg.normada\u001B[39;00m\n\u001B[1;32m   1091\u001B[0m     \u001B[38;5;66;03m# grads = [tf.clip_by_norm(g, 2) for g in grads]\u001B[39;00m\n\u001B[1;32m   1092\u001B[0m \n\u001B[1;32m   1093\u001B[0m     \u001B[38;5;66;03m# # The normalization of gradient of NGD can be hardcoded as\u001B[39;00m\n\u001B[1;32m   1094\u001B[0m     \u001B[38;5;66;03m# if isinstance(optimizer, NGD):\u001B[39;00m\n\u001B[1;32m   1095\u001B[0m     \u001B[38;5;66;03m#     grads = [g/tf.linalg.norm(g, 2) for g in grads]\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/CloudStorage/OneDrive-Personal/data-science/github/gisuelogit/src/aesuelogit/models.py:802\u001B[0m, in \u001B[0;36mGISUELOGIT.loss_function\u001B[0;34m(self, X, Y, lambdas, loss_metric)\u001B[0m\n\u001B[1;32m    799\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobserved_traveltimes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmask_observed_traveltimes(tt \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobserved_traveltimes, k \u001B[38;5;241m=\u001B[39m k)\n\u001B[1;32m    801\u001B[0m \u001B[38;5;66;03m# Under recurrent traffic conditions, we assume that the equilibrium flow and travel time is the same regardless the day  Thus, using self.flows() or self.traveltimes() is preferred.\u001B[39;00m\n\u001B[0;32m--> 802\u001B[0m predicted_flow \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_link_flows\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    803\u001B[0m predicted_traveltimes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbpr_traveltimes(predicted_flow)\n\u001B[1;32m    804\u001B[0m output_flow \u001B[38;5;241m=\u001B[39m predicted_flow\n",
      "File \u001B[0;32m~/Library/CloudStorage/OneDrive-Personal/data-science/github/gisuelogit/src/aesuelogit/models.py:1140\u001B[0m, in \u001B[0;36mGISUELOGIT.compute_link_flows\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m   1138\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_link_flows\u001B[39m(\u001B[38;5;28mself\u001B[39m,X):\n\u001B[0;32m-> 1140\u001B[0m     link_flows \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlink_flows(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpath_flows(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath_probabilities\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath_utilities\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlink_utilities\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m))\n\u001B[1;32m   1142\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mrank(link_flows) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m3\u001B[39m:\n\u001B[1;32m   1143\u001B[0m         link_flows \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mreduce_mean(link_flows,axis \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[0;32m~/Library/CloudStorage/OneDrive-Personal/data-science/github/gisuelogit/src/aesuelogit/models.py:595\u001B[0m, in \u001B[0;36mGISUELOGIT.path_probabilities\u001B[0;34m(self, vf, sparse_mode, normalization)\u001B[0m\n\u001B[1;32m    592\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpath_probabilities\u001B[39m(\u001B[38;5;28mself\u001B[39m, vf, sparse_mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, normalization\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[1;32m    594\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m sparse_mode:\n\u001B[0;32m--> 595\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath_probabilities_sparse\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnormalization\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnormalization\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    597\u001B[0m     \u001B[38;5;66;03m# TODO: Readapt this code to account for the hurs dimension or simply remove this as it is not used by default\u001B[39;00m\n\u001B[1;32m    598\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m normalization:\n",
      "File \u001B[0;32m~/Library/CloudStorage/OneDrive-Personal/data-science/github/gisuelogit/src/aesuelogit/models.py:577\u001B[0m, in \u001B[0;36mGISUELOGIT.path_probabilities_sparse\u001B[0;34m(self, vf, normalization)\u001B[0m\n\u001B[1;32m    570\u001B[0m M_sparse \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39msparse\u001B[38;5;241m.\u001B[39mconcat(\u001B[38;5;241m0\u001B[39m, [tf\u001B[38;5;241m.\u001B[39msparse\u001B[38;5;241m.\u001B[39mexpand_dims(M_sparse, \u001B[38;5;241m0\u001B[39m)] \u001B[38;5;241m*\u001B[39m vf\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m])\n\u001B[1;32m    572\u001B[0m indices \u001B[38;5;241m=\u001B[39m M_sparse\u001B[38;5;241m.\u001B[39mindices\n\u001B[1;32m    574\u001B[0m V \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39msparse\u001B[38;5;241m.\u001B[39mSparseTensor(indices\u001B[38;5;241m=\u001B[39mindices,\n\u001B[1;32m    575\u001B[0m                            \u001B[38;5;66;03m# values = tf.exp(tf.reshape(vf,-1)),\u001B[39;00m\n\u001B[1;32m    576\u001B[0m                            values\u001B[38;5;241m=\u001B[39mtf\u001B[38;5;241m.\u001B[39mreshape(vf, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m),\n\u001B[0;32m--> 577\u001B[0m                            dense_shape\u001B[38;5;241m=\u001B[39m(vf\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], vf\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m], \u001B[38;5;241m*\u001B[39m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mM\u001B[49m\u001B[38;5;241m.\u001B[39mshape))\n\u001B[1;32m    579\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m normalization:\n\u001B[1;32m    580\u001B[0m     normalized_values \u001B[38;5;241m=\u001B[39m V\u001B[38;5;241m.\u001B[39mvalues \u001B[38;5;241m-\u001B[39m tf\u001B[38;5;241m.\u001B[39mreshape(\n\u001B[1;32m    581\u001B[0m         tf\u001B[38;5;241m.\u001B[39meinsum(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mijk,kl -> ijl\u001B[39m\u001B[38;5;124m\"\u001B[39m, tf\u001B[38;5;241m.\u001B[39mstop_gradient(tf\u001B[38;5;241m.\u001B[39msparse\u001B[38;5;241m.\u001B[39mreduce_max(V, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m)), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mM), \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m~/Library/CloudStorage/OneDrive-Personal/data-science/github/gisuelogit/src/aesuelogit/models.py:468\u001B[0m, in \u001B[0;36mGISUELOGIT.M\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    466\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[1;32m    467\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mM\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m--> 468\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconstant\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnetwork\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mM\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/aesuelogit-1kW0OCrg-py3.9/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:267\u001B[0m, in \u001B[0;36mconstant\u001B[0;34m(value, dtype, shape, name)\u001B[0m\n\u001B[1;32m    170\u001B[0m \u001B[38;5;129m@tf_export\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconstant\u001B[39m\u001B[38;5;124m\"\u001B[39m, v1\u001B[38;5;241m=\u001B[39m[])\n\u001B[1;32m    171\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mconstant\u001B[39m(value, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, shape\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConst\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    172\u001B[0m   \u001B[38;5;124;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001B[39;00m\n\u001B[1;32m    173\u001B[0m \n\u001B[1;32m    174\u001B[0m \u001B[38;5;124;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    265\u001B[0m \u001B[38;5;124;03m    ValueError: if called on a symbolic tensor.\u001B[39;00m\n\u001B[1;32m    266\u001B[0m \u001B[38;5;124;03m  \"\"\"\u001B[39;00m\n\u001B[0;32m--> 267\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_constant_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverify_shape\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    268\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mallow_broadcast\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/aesuelogit-1kW0OCrg-py3.9/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:279\u001B[0m, in \u001B[0;36m_constant_impl\u001B[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001B[0m\n\u001B[1;32m    277\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m trace\u001B[38;5;241m.\u001B[39mTrace(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtf.constant\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    278\u001B[0m       \u001B[38;5;28;01mreturn\u001B[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001B[0;32m--> 279\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_constant_eager_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverify_shape\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    281\u001B[0m g \u001B[38;5;241m=\u001B[39m ops\u001B[38;5;241m.\u001B[39mget_default_graph()\n\u001B[1;32m    282\u001B[0m tensor_value \u001B[38;5;241m=\u001B[39m attr_value_pb2\u001B[38;5;241m.\u001B[39mAttrValue()\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/aesuelogit-1kW0OCrg-py3.9/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:304\u001B[0m, in \u001B[0;36m_constant_eager_impl\u001B[0;34m(ctx, value, dtype, shape, verify_shape)\u001B[0m\n\u001B[1;32m    302\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_constant_eager_impl\u001B[39m(ctx, value, dtype, shape, verify_shape):\n\u001B[1;32m    303\u001B[0m   \u001B[38;5;124;03m\"\"\"Creates a constant on the current device.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 304\u001B[0m   t \u001B[38;5;241m=\u001B[39m \u001B[43mconvert_to_eager_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    305\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m shape \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    306\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/aesuelogit-1kW0OCrg-py3.9/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:102\u001B[0m, in \u001B[0;36mconvert_to_eager_tensor\u001B[0;34m(value, ctx, dtype)\u001B[0m\n\u001B[1;32m    100\u001B[0m     dtype \u001B[38;5;241m=\u001B[39m dtypes\u001B[38;5;241m.\u001B[39mas_dtype(dtype)\u001B[38;5;241m.\u001B[39mas_datatype_enum\n\u001B[1;32m    101\u001B[0m ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[0;32m--> 102\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mEagerTensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "if run_model['odlue']:\n",
    "\n",
    "    print('\\nODLUE: OD + utility estimation with historic OD')\n",
    "\n",
    "    # _RELATIVE_GAP = 1e-4\\\n",
    "    # _XTICKS_SPACING = 50\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=_LR)\n",
    "    # optimizer = tf.keras.optimizers.Adagrad(learning_rate=_LR)\n",
    "    utility_parameters = UtilityParameters(features_Y=['tt'],\n",
    "                                           features_Z=features_Z,\n",
    "                                           periods=1,\n",
    "                                           initial_values={'psc_factor': 0,\n",
    "                                                           'fixed_effect': np.zeros_like(fresno_network.links)},\n",
    "                                           signs={'tt': '-', 'speed_sd': '-', 'median_inc': '+', 'incidents': '-',\n",
    "                                                  'bus_stops': '-', 'intersections': '-'},\n",
    "                                           trainables={'psc_factor': False, 'fixed_effect': True,\n",
    "                                                       'tt': True, 'speed_sd': True, 'median_inc': True,\n",
    "                                                       'incidents': True,\n",
    "                                                       'bus_stops': True, 'intersections': True\n",
    "                                                       },\n",
    "                                           )\n",
    "\n",
    "    bpr_parameters = BPRParameters(keys=['alpha', 'beta'],\n",
    "                                   initial_values={'alpha': 0.15, 'beta': 4},\n",
    "                                   trainables=dict.fromkeys(['alpha', 'beta'], False),\n",
    "                                   )\n",
    "\n",
    "    od_parameters = ODParameters(key='od',\n",
    "                                 periods=1,\n",
    "                                 initial_values=fresno_network.q.flatten(),\n",
    "                                 historic_values={1: fresno_network.q.flatten()},\n",
    "                                 trainable=True)\n",
    "\n",
    "    equilibrator = Equilibrator(\n",
    "        network=fresno_network,\n",
    "        # paths_generator=paths_generator,\n",
    "        utility=utility_parameters,\n",
    "        max_iters=100,\n",
    "        method='fw',\n",
    "        iters_fw=50,\n",
    "        accuracy=1e-4,\n",
    "    )\n",
    "\n",
    "    column_generator = ColumnGenerator(equilibrator=equilibrator,\n",
    "                                       utility=utility_parameters,\n",
    "                                       n_paths=0,\n",
    "                                       ods_coverage=0.1,\n",
    "                                       ods_sampling='sequential',\n",
    "                                       # ods_sampling='demand',\n",
    "                                       )\n",
    "\n",
    "    odlue = GISUELOGIT(\n",
    "        key='odlue',\n",
    "        network=fresno_network,\n",
    "        dtype=tf.float64,\n",
    "        equilibrator=equilibrator,\n",
    "        column_generator=column_generator,\n",
    "        utility=utility_parameters,\n",
    "        bpr=bpr_parameters,\n",
    "        od=od_parameters,\n",
    "    )\n",
    "\n",
    "    train_results_dfs['odlue'], test_results_dfs['odlue'] = odlue.train(\n",
    "        X_train, Y_train, X_test, Y_test,\n",
    "        optimizer=optimizer,\n",
    "        batch_size=_BATCH_SIZE,\n",
    "        # generalization_error={'train': False, 'validation': True},\n",
    "        loss_weights= _LOSS_WEIGHTS,\n",
    "        loss_metric=_LOSS_METRIC,\n",
    "        threshold_relative_gap=_RELATIVE_GAP,\n",
    "        epochs_print_interval=_EPOCHS_PRINT_INTERVAL,\n",
    "        epochs=_EPOCHS)\n",
    "\n",
    "    plot_predictive_performance(train_losses=train_results_dfs['odlue'], val_losses=test_results_dfs['odlue'],\n",
    "                                xticks_spacing = _XTICKS_SPACING)\n",
    "\n",
    "    print(f\"theta = {dict(zip(utility_parameters.true_values.keys(), list(odlue.theta.numpy())))}\")\n",
    "    print(f\"alpha = {odlue.alpha: 0.2f}, beta  = {odlue.beta: 0.2f}\")\n",
    "    print(f\"Avg abs diff of observed and estimated OD: {np.mean(np.abs(odlue.q - fresno_network.q.flatten())): 0.2f}\")\n",
    "    print(f\"Avg observed OD: {np.mean(np.abs(fresno_network.q.flatten())): 0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: ODLUE + link specific performance parameters (only alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_model['odlulpe-1']:\n",
    "\n",
    "    print('\\nODLULPE: ODLUE + link performance parameters with historic OD matrix (link specifics alpha)')\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=_LR)\n",
    "\n",
    "    # _LR = 1e-2\n",
    "    # _RELATIVE_GAP = 1e-5\n",
    "    # _XTICKS_SPACING = 50\n",
    "\n",
    "    # Some initializations of the bpr parameters, makes the optimization to fail (e.g. alpha =1, beta = 1). Using a common\n",
    "    # alpha but different betas for every link make the estimation more stable but there is overfitting after a certain amount of iterations\n",
    "\n",
    "    bpr_parameters = BPRParameters(keys=['alpha', 'beta'],\n",
    "                                   # initial_values={'alpha': 0.15*np.ones_like(fresno_network.links,dtype = np.float32),\n",
    "                                   #                 'beta': 4*np.ones_like(fresno_network.links,dtype = np.float32)},\n",
    "                                   initial_values={'alpha': 0.15*np.ones_like(fresno_network.links,dtype = np.float32),\n",
    "                                                   'beta': 4},\n",
    "                                   # initial_values={'alpha': 0.15, 'beta': 4},\n",
    "                                   # initial_values={'alpha': 0.15,\n",
    "                                   #                 'beta': 4 * np.ones_like(fresno_network.links, dtype=np.float32)},\n",
    "                                   # initial_values={'alpha': 0.15, 'beta': 4},\n",
    "                                   trainables={'alpha': True, 'beta':False},\n",
    "                                   # trainables={'alpha': True, 'beta': True},\n",
    "                                   # trainables={'alpha': False, 'beta': True},\n",
    "                                   )\n",
    "\n",
    "    od_parameters = ODParameters(key='od',\n",
    "                                 periods=1,\n",
    "                                 initial_values=fresno_network.q.flatten(),\n",
    "                                 historic_values={1: fresno_network.q.flatten()},\n",
    "                                 trainable=True)\n",
    "\n",
    "    utility_parameters = UtilityParameters(features_Y=['tt'],\n",
    "                                           features_Z=features_Z,\n",
    "                                           periods=1,\n",
    "                                           initial_values={'psc_factor': 0, 'tt':0,\n",
    "                                                           'fixed_effect': np.zeros_like(fresno_network.links)},\n",
    "                                           signs={'tt': '-', 'speed_sd': '-', 'median_inc': '+', 'incidents': '-',\n",
    "                                                  'bus_stops': '-', 'intersections': '-'},\n",
    "                                           trainables={'psc_factor': False, 'fixed_effect': True,\n",
    "                                                       'tt': True, 'speed_sd': True, 'median_inc': True,\n",
    "                                                       'incidents': True,\n",
    "                                                       'bus_stops': True, 'intersections': True\n",
    "                                                       },\n",
    "                                           )\n",
    "\n",
    "    equilibrator = Equilibrator(\n",
    "        network=fresno_network,\n",
    "        # paths_generator=paths_generator,\n",
    "        utility=utility_parameters,\n",
    "        max_iters=100,\n",
    "        method='fw',\n",
    "        iters_fw=50,\n",
    "        accuracy=1e-4,\n",
    "    )\n",
    "\n",
    "    column_generator = ColumnGenerator(equilibrator=equilibrator,\n",
    "                                       utility=utility_parameters,\n",
    "                                       n_paths=0,\n",
    "                                       ods_coverage=0.1,\n",
    "                                       ods_sampling='sequential',\n",
    "                                       # ods_sampling='demand',\n",
    "                                       )\n",
    "\n",
    "    odlulpe_1 = GISUELOGIT(\n",
    "        key='odlulpe-1',\n",
    "        network=fresno_network,\n",
    "        dtype=tf.float64,\n",
    "        equilibrator=equilibrator,\n",
    "        column_generator=column_generator,\n",
    "        utility=utility_parameters,\n",
    "        bpr=bpr_parameters,\n",
    "        od=od_parameters,\n",
    "    )\n",
    "\n",
    "    train_results_dfs['odlulpe-1'], test_results_dfs['odlulpe-1'] = odlulpe_1.train(\n",
    "        X_train, Y_train, X_test, Y_test,\n",
    "        optimizer=optimizer,\n",
    "        # generalization_error={'train': False, 'validation': True},\n",
    "        batch_size=_BATCH_SIZE,\n",
    "        loss_weights=_LOSS_WEIGHTS,\n",
    "        loss_metric=_LOSS_METRIC,\n",
    "        threshold_relative_gap=_RELATIVE_GAP,\n",
    "        epochs_print_interval=_EPOCHS_PRINT_INTERVAL,\n",
    "        epochs=_EPOCHS)\n",
    "\n",
    "    plot_predictive_performance(train_losses=train_results_dfs['odlulpe-1'], val_losses=test_results_dfs['odlulpe-1'],\n",
    "                                xticks_spacing = _XTICKS_SPACING)\n",
    "\n",
    "    print(f\"theta = {dict(zip(utility_parameters.true_values.keys(), list(odlulpe_1.theta.numpy())))}\")\n",
    "    print(f\"alpha = {np.mean(odlulpe_1.alpha): 0.2f}, beta  = {np.mean(odlulpe_1.beta): 0.2f}\")\n",
    "    print(f\"Avg abs diff of observed and estimated OD: {np.mean(np.abs(odlulpe_1.q - fresno_network.q.flatten())): 0.2f}\")\n",
    "    print(f\"Avg observed OD: {np.mean(np.abs(fresno_network.q.flatten())): 0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5: ODLUE + link specific performance parameters (alphas and betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_model['odlulpe-2']:\n",
    "\n",
    "    print('\\nODLULPE: ODLUE + link performance parameters with historic OD matrix (link specifics alphas and betas)')\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=_LR)\n",
    "\n",
    "    # _LR = 5e-1\n",
    "    # _RELATIVE_GAP = 1e-5\n",
    "\n",
    "    # Some initializations of the bpr parameters, makes the optimization to fail (e.g. alpha =1, beta = 1). Using a common\n",
    "    # alpha but different betas for every link make the estimation more stable but there is overfitting after a certain amount of iterations\n",
    "\n",
    "    bpr_parameters = BPRParameters(keys=['alpha', 'beta'],\n",
    "                                   initial_values={'alpha': 0.15*np.ones_like(fresno_network.links,dtype = np.float32),\n",
    "                                                   'beta': 4*np.ones_like(fresno_network.links,dtype = np.float32)},\n",
    "                                   # initial_values={'alpha': 0.15*np.ones_like(fresno_network.links,dtype = np.float32),\n",
    "                                   #                 'beta': 4},\n",
    "                                   # initial_values={'alpha': 0.15, 'beta': 4},\n",
    "                                   # initial_values={'alpha': 0.15,\n",
    "                                   #                 'beta': 4 * np.ones_like(fresno_network.links, dtype=np.float32)},\n",
    "                                   # initial_values={'alpha': 0.15, 'beta': 4},\n",
    "                                   # trainables={'alpha': True, 'beta':False},\n",
    "                                   trainables={'alpha': True, 'beta': True},\n",
    "                                   # trainables={'alpha': False, 'beta': True},\n",
    "                                   )\n",
    "\n",
    "    od_parameters = ODParameters(key='od',\n",
    "                                 periods=1,\n",
    "                                 initial_values=fresno_network.q.flatten(),\n",
    "                                 historic_values={1: fresno_network.q.flatten()},\n",
    "                                 trainable=True)\n",
    "\n",
    "    utility_parameters = UtilityParameters(features_Y=['tt'],\n",
    "                                           features_Z=features_Z,\n",
    "                                           periods=1,\n",
    "                                           initial_values={'psc_factor': 0, 'tt':0,\n",
    "                                                           'fixed_effect': np.zeros_like(fresno_network.links)},\n",
    "                                           signs={'tt': '-', 'speed_sd': '-', 'median_inc': '+', 'incidents': '-',\n",
    "                                                  'bus_stops': '-', 'intersections': '-'},\n",
    "                                           trainables={'psc_factor': False, 'fixed_effect': True,\n",
    "                                                       'tt': True, 'speed_sd': True, 'median_inc': True,\n",
    "                                                       'incidents': True,\n",
    "                                                       'bus_stops': True, 'intersections': True\n",
    "                                                       },\n",
    "                                           )\n",
    "\n",
    "    equilibrator = Equilibrator(\n",
    "        network=fresno_network,\n",
    "        # paths_generator=paths_generator,\n",
    "        utility=utility_parameters,\n",
    "        max_iters=100,\n",
    "        method='fw',\n",
    "        iters_fw=50,\n",
    "        accuracy=1e-4,\n",
    "    )\n",
    "\n",
    "    column_generator = ColumnGenerator(equilibrator=equilibrator,\n",
    "                                       utility=utility_parameters,\n",
    "                                       n_paths=0,\n",
    "                                       ods_coverage=0.1,\n",
    "                                       ods_sampling='sequential',\n",
    "                                       # ods_sampling='demand',\n",
    "                                       )\n",
    "\n",
    "    odlulpe_2 = GISUELOGIT(\n",
    "        key='odlulpe_2',\n",
    "        network=fresno_network,\n",
    "        dtype=tf.float64,\n",
    "        equilibrator=equilibrator,\n",
    "        column_generator=column_generator,\n",
    "        utility=utility_parameters,\n",
    "        bpr=bpr_parameters,\n",
    "        od=od_parameters,\n",
    "    )\n",
    "\n",
    "    train_results_dfs['odlulpe_2'], test_results_dfs['odlulpe_2'] = odlulpe_2.train(\n",
    "        X_train, Y_train, X_test, Y_test,\n",
    "        optimizer=optimizer,\n",
    "        # generalization_error={'train': False, 'validation': True},\n",
    "        batch_size=_BATCH_SIZE,\n",
    "        # loss_weights={'od': 1, 'theta': 0, 'tt': 1, 'flow': 1, 'eq_flow': 1},\n",
    "        loss_weights=_LOSS_WEIGHTS,\n",
    "        threshold_relative_gap=_RELATIVE_GAP,\n",
    "        epochs_print_interval=_EPOCHS_PRINT_INTERVAL,\n",
    "        loss_metric=_LOSS_METRIC,\n",
    "        epochs=_EPOCHS)\n",
    "\n",
    "    plot_predictive_performance(train_losses=train_results_dfs['odlulpe_2'], val_losses=test_results_dfs['odlulpe_2'],\n",
    "                                xticks_spacing = _XTICKS_SPACING)\n",
    "\n",
    "    print(f\"theta = {dict(zip(utility_parameters.true_values.keys(), list(odlulpe_2.theta.numpy())))}\")\n",
    "    print(f\"alpha = {np.mean(odlulpe_2.alpha): 0.2f}, beta  = {np.mean(odlulpe_2.beta): 0.2f}\")\n",
    "    print(f\"Avg abs diff of observed and estimated OD: {np.mean(np.abs(odlulpe_2.q - fresno_network.q.flatten())): 0.2f}\")\n",
    "    print(f\"Avg observed OD: {np.mean(np.abs(fresno_network.q.flatten())): 0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aesuelogit-1kW0OCrg-py3.9",
   "language": "python",
   "name": "aesuelogit-1kw0ocrg-py3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
